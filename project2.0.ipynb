{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Using cached clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Using cached emoji-1.7.0.tar.gz (175 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\marku\\appdata\\roaming\\python\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n",
      "Installing collected packages: emoji, ftfy, clean-text\n",
      "  Running setup.py install for emoji: started\n",
      "  Running setup.py install for emoji: finished with status 'done'\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: emoji is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install clean-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp311-cp311-win_amd64.whl (267 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\marku\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\marku\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Collecting numpy>=1.21.0\n",
      "  Using cached numpy-1.24.2-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marku\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.24.2 pandas-1.5.3 pytz-2022.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sometimes, the, power, of, christmas, will, m...\n",
      "1      [awakening, of, number, strands, of, dna, reco...\n",
      "2      [never, hike, alone, a, friday, the, 13th, fan...\n",
      "3      [when, a, rare, shark, was, caught, scientists...\n",
      "4      [donald, trump, has, the, unnerving, ability, ...\n",
      "                             ...                        \n",
      "245    [prison, for, rahm, gods, work, and, many, oth...\n",
      "246    [number, useful, items, for, your, tiny, home,...\n",
      "247    [former, cia, director, michael, hayden, said,...\n",
      "248    [antonio, sabato, jr, says, hollywoods, libera...\n",
      "249    [former, us, president, bill, clinton, on, mon...\n",
      "Name: content, Length: 250, dtype: object\n",
      "Stop words length:  179\n",
      "Stop words dictionary:  {'ain', \"weren't\", 'but', 'yours', 'did', 'which', 'that', 'from', 'than', 'at', 'do', \"won't\", \"hadn't\", 'the', 'no', 'does', 'myself', \"that'll\", \"you'll\", \"isn't\", 'o', 'what', 'only', 'by', 'if', 'was', \"you've\", 'can', 'were', 'them', 'down', 'before', 'below', 'those', 'own', 'we', 'same', 'who', 'shouldn', 's', 'being', 're', 'hasn', 'her', 'more', 'should', \"wasn't\", \"aren't\", 'ma', 'just', 'y', 'will', 'isn', \"you'd\", 'these', 'each', 'wasn', 'because', 'while', 'mightn', 'had', 'didn', 'nor', 'to', 'why', 'now', \"shouldn't\", 'between', \"you're\", \"it's\", 've', 'very', 'have', 'you', \"didn't\", \"should've\", 'then', 'or', 'too', 'as', 'such', 'having', \"haven't\", 'and', \"mightn't\", 'of', 'all', 'yourselves', 'theirs', 'into', 'it', 'my', 'out', 'd', 'won', 'again', 'his', \"shan't\", 'yourself', 'here', 'him', 'through', 'under', 'a', 'after', \"mustn't\", 'aren', 'ours', 'i', 'he', 'don', 'in', \"wouldn't\", 'be', 'some', 'themselves', 'above', 'over', 'this', \"couldn't\", 'on', 'both', 'herself', 'until', 'their', 'an', 'she', 'ourselves', 'has', 'haven', 'himself', 'they', \"hasn't\", 'once', 'about', 'wouldn', 'll', 'couldn', 'whom', 'when', 'with', 'where', 'not', 'hers', 'how', 'so', 'there', 'itself', 'few', \"needn't\", 'doesn', 't', \"doesn't\", \"don't\", 'doing', 'off', 'hadn', 'mustn', \"she's\", 'most', 'other', 'shan', 'weren', 'during', 'any', 'up', 'needn', 'for', 'is', 'are', 'its', 'against', 'm', 'our', 'your', 'been', 'further', 'am', 'me'}\n",
      "Difference between number of tokens before and after removing stopwords:  133\n",
      "Number of words after tokenizing:  16641\n",
      "Number of words after removing stopwords:  16508\n",
      "Reduction rate after removing stopwords:  0.7992308154558003\n",
      "Number of words after stemming:  10998\n",
      "Reduction rate after stemming:  33.377756239399076\n"
     ]
    }
   ],
   "source": [
    "#################################### PACKAGES ######################################\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "R = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\") \n",
    "\n",
    "\n",
    "####################################### CLEANING ######################################\n",
    "Copy_clean = R.copy()\n",
    "\n",
    "def cleaningTime(x):\n",
    "    clean_text = x.replace('|', '')\n",
    "    clean_text = clean_text.replace('$', '')\n",
    "    regex = r'[xX]+'\n",
    "    clean_text = re.sub(regex, '', clean_text)\n",
    "    # Replace date with DATE\n",
    "    regex = r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}'\n",
    "    clean_text = re.sub(regex, 'DATE', clean_text)\n",
    "    clean_text = clean(clean_text,\n",
    "        no_punct=True,\n",
    "        lower=True,                    # lowercase text\n",
    "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,\n",
    "        replace_with_url=\"url\",\n",
    "        replace_with_email=\"email\",\n",
    "        replace_with_number=\"number\",\n",
    "        )\n",
    "    return clean_text\n",
    "\n",
    "#R['content'].apply(cleaning)\n",
    "Copy_clean['content'] = R['content'].apply(cleaningTime)\n",
    "#print(Copy_clean['content'])\n",
    "\n",
    "# (r'C:\\Users\\karen\\nein.csv', index= None, header=True)\n",
    "df = pd.DataFrame(Copy_clean)\n",
    "df.to_csv('copy_clean.csv', index= None, header=True)\n",
    "\n",
    "\n",
    "####################################### TOKENIZE #########################################\n",
    "Copy_tok = Copy_clean.copy()\n",
    "\n",
    "def Tokenize(x):     \n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "Copy_tok['content'] = Copy_tok['content'].apply(Tokenize)\n",
    "print(Copy_tok['content'])\n",
    "\n",
    "#Copy_stop = []\n",
    "#for i in Copy_tok['content']:\n",
    "    #tokens = nltk.word_tokenize(i)\n",
    "    #Copy_stop.append(tokens)\n",
    "#print(Copy_stop)\n",
    "\n",
    "######################################## REMOVING STOPWORDS ################################\n",
    "Copy_stop = Copy_tok.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop words length: \", len(stop_words))\n",
    "print(\"Stop words dictionary: \", (stop_words))\n",
    "# removing\n",
    "def Remove_stopwords(x): # x = Copy_tok['content']\n",
    "    filtered_sentence = []\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "Copy_stop['content'] = Copy_stop['content'].apply(Remove_stopwords)\n",
    "#print(Copy_stop['content'])\n",
    "\n",
    "\n",
    "######################################### STEMMING ##########################################\n",
    "Copy_stemming = Copy_stop.copy()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def Stemming_Words(x):\n",
    "    stem_words = []\n",
    "    for w in x:\n",
    "        word = stemmer.stem(w)\n",
    "        stem_words.append(word)\n",
    "    return stem_words\n",
    "Copy_stemming['content'] = Copy_stemming['content'].apply(Stemming_Words)\n",
    "#print(Copy_stemming['content'])\n",
    "\n",
    "##############################################################################################\n",
    "# number of tokens before removing stopwords\n",
    "def unique_words (file):\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return len(UN)\n",
    "\n",
    "\n",
    "# Difference between number of tokens before and after removing stopwords\n",
    "print(\"Difference between number of tokens before and after removing stopwords: \", (unique_words(Copy_tok['content'])-unique_words(Copy_stop['content'])))\n",
    "\n",
    "# number of tokens after tokenizing\n",
    "print(\"Number of words after tokenizing: \", unique_words(Copy_tok['content']))\n",
    "\n",
    "# number of tokens after removing stopwords\n",
    "print(\"Number of words after removing stopwords: \", unique_words(Copy_stop['content']))\n",
    "\n",
    "# reduction rate after removing stopwords\n",
    "print(\"Reduction rate after removing stopwords: \", 100 - unique_words(Copy_stop['content'])/unique_words(Copy_tok['content']) * 100)\n",
    "\n",
    "# number of tokens after stemming\n",
    "print(\"Number of words after stemming: \", unique_words(Copy_stemming['content']))\n",
    "\n",
    "# reduction rate after stemming\n",
    "print(\"Reduction rate after stemming: \", 100 - unique_words(Copy_stemming['content'])/unique_words(Copy_stop['content']) * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The URLs are not correctly formatted: Each URL includes the first word in the content!!!\n",
    "* Not sure whether the counts below are done in the right order\n",
    "* Regarding dates: We need to choose which column to use: (1) scraped_at, (2) inserted_at, (3) updated_at\n",
    "* Insert more \"replace\" statements in CleaningTime (i.e. DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs:  242\n",
      "Number of dates:  27\n",
      "Number of numeric values:  2019\n",
      "Number of punctuations:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # COUNTING\n",
    "# count = 0 \n",
    "# wordlist = ['url', 'number']\n",
    "# for i in Copy_stemming['content']:\n",
    "#     for x in i:\n",
    "#         if x in wordlist:\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             count = count\n",
    "# #count\n",
    "\n",
    "\n",
    "# Count URLs\n",
    "count_url = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'url':\n",
    "            count_url += 1\n",
    "        else:\n",
    "            count_url = count_url\n",
    "print(\"Number of URLs: \", count_url)\n",
    "\n",
    "# Count dates\n",
    "count_date = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'date':\n",
    "            count_date += 1\n",
    "        else:\n",
    "            count_date = count_date\n",
    "print(\"Number of dates: \", count_date)\n",
    "\n",
    "# Count numbers (numeric values)\n",
    "count_number = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'number':\n",
    "            count_number += 1\n",
    "        else:\n",
    "            count_number = count_number\n",
    "print(\"Number of numeric values: \", count_number)\n",
    "\n",
    "# Count punctuation: check if punctuations are removed\n",
    "count_punct = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'punct':\n",
    "            count_punct += 1\n",
    "        else:\n",
    "            count_punct = count_punct\n",
    "print(\"Number of punctuations (check): \", count_punct)\n",
    "\n",
    "# # Print the date of the oldest article by using 'inserted_at' column\n",
    "# print(\"The date of the oldest article: \", min(R['inserted_at']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "900b9bb925fcdb02ec6c2453d36a87d904f6c7d996c9903f56d484ca6ecf1666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
