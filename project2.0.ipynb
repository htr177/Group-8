{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: C:\\Users\\karen\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install clean-text\n",
    "# Hej Karen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: C:\\Users\\karen\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sometimes, the, power, of, christmas, will, m...\n",
      "1      [awakening, of, number, strands, of, dna, reco...\n",
      "2      [never, hike, alone, a, friday, the, 13th, fan...\n",
      "3      [when, a, rare, shark, was, caught, scientists...\n",
      "4      [donald, trump, has, the, unnerving, ability, ...\n",
      "                             ...                        \n",
      "245    [prison, for, rahm, gods, work, and, many, oth...\n",
      "246    [number, useful, items, for, your, tiny, home,...\n",
      "247    [former, cia, director, michael, hayden, said,...\n",
      "248    [antonio, sabato, jr, says, hollywoods, libera...\n",
      "249    [former, us, president, bill, clinton, on, mon...\n",
      "Name: content, Length: 250, dtype: object\n",
      "179\n",
      "16641\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "#################################### PACKAGES ######################################\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "R = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\") \n",
    "\n",
    "\n",
    "####################################### CLEANING ######################################\n",
    "Copy_clean = R.copy()\n",
    "\n",
    "def cleaningTime(x):\n",
    "    clean_text = x.replace('|', '')\n",
    "    clean_text = clean_text.replace('$', '')\n",
    "    regex = r'[xX]+'\n",
    "    clean_text = re.sub(regex, '', clean_text)\n",
    "    #clean_text = x.replace(',', '')\n",
    "    clean_text = clean(clean_text,\n",
    "        no_punct=True,\n",
    "        lower=True,                    # lowercase text\n",
    "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,\n",
    "        replace_with_url=\"URL\",\n",
    "        replace_with_email=\"EMAIL\",\n",
    "        replace_with_number=\"NUMBER\",\n",
    "        )\n",
    "    return clean_text\n",
    "\n",
    "#R['content'].apply(cleaning)\n",
    "Copy_clean['content'] = R['content'].apply(cleaningTime)\n",
    "#print(Copy_clean['content'])\n",
    "df = pd.DataFrame(Copy_clean)\n",
    "df.to_csv(r'C:\\Users\\karen\\nein.csv', index= None, header=True)\n",
    "\n",
    "\n",
    "####################################### TOKENIZE #########################################\n",
    "Copy_tok = Copy_clean.copy()\n",
    "\n",
    "def Tokenize(x):     \n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "Copy_tok['content'] = Copy_tok['content'].apply(Tokenize)\n",
    "print(Copy_tok['content'])\n",
    "\n",
    "#Copy_stop = []\n",
    "#for i in Copy_tok['content']:\n",
    "    #tokens = nltk.word_tokenize(i)\n",
    "    #Copy_stop.append(tokens)\n",
    "#print(Copy_stop)\n",
    "\n",
    "######################################## REMOVING STOPWORDS ################################\n",
    "Copy_stop = Copy_tok.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))\n",
    "\n",
    "# removing\n",
    "def Remove_stopwords(x): # x = Copy_tok['content']\n",
    "    filtered_sentence = []\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "Copy_stop['content'] = Copy_stop['content'].apply(Remove_stopwords)\n",
    "#print(Copy_stop['content'])\n",
    "\n",
    "\n",
    "######################################### STEMMING ##########################################\n",
    "Copy_stemming = Copy_stop.copy()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def Stemming_Words(x):\n",
    "    stem_words = []\n",
    "    for w in x:\n",
    "        word = stemmer.stem(w)\n",
    "        stem_words.append(word)\n",
    "    return stem_words\n",
    "Copy_stemming['content'] = Copy_stemming['content'].apply(Stemming_Words)\n",
    "#print(Copy_stemming['content'])\n",
    "\n",
    "##############################################################################################\n",
    "# number of tokens before removing stopwords\n",
    "def unique_words (file):\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return len(UN)\n",
    "\n",
    "print(unique_words(Copy_tok['content']))\n",
    "####\n",
    "print(unique_words(Copy_tok['content'])-unique_words(Copy_stop['content']))\n",
    "\n",
    "# reductionrate - from token to after removing stopwords \n",
    "x = 100 - 96127/169480 * 100 # = 43.28%\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COUNTING\n",
    "count = 0 \n",
    "wordlist = ['url', 'number']\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x in wordlist:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = count\n",
    "#count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "900b9bb925fcdb02ec6c2453d36a87d904f6c7d996c9903f56d484ca6ecf1666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
