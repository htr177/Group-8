{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#################################### PACKAGES ######################################\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "#R = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\") \n",
    "\n",
    "\n",
    "####################################### CLEANING ######################################\n",
    "Copy_clean = R.copy()\n",
    "\n",
    "def cleaningTime(x):\n",
    "    if isinstance(x, str):\n",
    "        clean_text = x.replace('|', '')\n",
    "        clean_text = clean_text.replace('+', '')\n",
    "        clean_text = clean_text.replace('$', '')\n",
    "        \n",
    "        regex = r'\\b[xX]+\\b'\n",
    "        regex2 = r'[=]+'\n",
    "        clean_text = re.sub(regex, '', clean_text)\n",
    "        clean_text = re.sub(regex2, '', clean_text)\n",
    "        clean_text = re.sub('<', ' ', clean_text)\n",
    "        clean_text = re.sub('>', ' ', clean_text)\n",
    "        \n",
    "        clean_text = re.sub(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}', '<DATE>', clean_text)\n",
    "        \n",
    "        clean_text = clean(clean_text,\n",
    "            no_punct=True,\n",
    "            lower=True,                    # lowercase text\n",
    "            no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "            no_urls=True,                  # replace all URLs with a special token\n",
    "            no_emails=True,                # replace all email addresses with a special token\n",
    "            no_numbers=True,\n",
    "            replace_with_url=\"<URL>\",\n",
    "            replace_with_email=\"<EMAIL>\",\n",
    "            replace_with_number=\"<NUMBER>\",\n",
    "            )\n",
    "        clean_text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', clean_text)\n",
    "        return clean_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#R['content'].apply(cleaning)\n",
    "#Copy_clean['content'] = R['content'].apply(cleaningTime)\n",
    "#print(Copy_clean['content'])\n",
    "#df = pd.DataFrame(Copy_clean)\n",
    "#df.to_csv(r'C:\\Users\\karen\\nein.csv', index= None, header=True)\n",
    "\n",
    "\n",
    "####################################### TOKENIZE #########################################\n",
    "#Copy_tok = Copy_clean.copy()\n",
    "\n",
    "def Tokenize(x):     \n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "#Copy_tok['content'] = Copy_tok['content'].apply(Tokenize)\n",
    "#print(Copy_tok['content'])\n",
    "\n",
    "\n",
    "######################################## REMOVING STOPWORDS ################################\n",
    "#Copy_stop = Copy_tok.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"Number of english stopwords: \", len(stop_words))\n",
    "\n",
    "# removing\n",
    "def Remove_stopwords(x): \n",
    "    filtered_sentence = []\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "#Copy_stop['content'] = Copy_stop['content'].apply(Remove_stopwords)\n",
    "\n",
    "\n",
    "######################################### STEMMING ##########################################\n",
    "#Copy_stemming = Copy_stop.copy()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def Stemming_Words(x):\n",
    "    stem_words = []\n",
    "    for w in x:\n",
    "        word = stemmer.stem(w)\n",
    "        stem_words.append(word)\n",
    "    return stem_words\n",
    "#Copy_stemming['content'] = Copy_stemming['content'].apply(Stemming_Words)\n",
    "\n",
    "\n",
    "######################################## COUNTING WORDS #####################################\n",
    "\n",
    "def unique_words (file):\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return UN\n",
    "\n",
    "# number of unique words after...\n",
    "#print(\"Number of unique words after tokenization: \", len(unique_words(Copy_tok['content'])))\n",
    "#print(\"Number of unique words after removing stopwords: \", len(unique_words(Copy_stop['content'])))\n",
    "#print(\"Number of unique words after stemming: \", len(unique_words(Copy_stemming['content'])))\n",
    "\n",
    "# reductionrates\n",
    "#print(\"Reduction of UW from tokenization to after removing stopwords (in %): \", (len(unique_words(Copy_stop['content']))/len(unique_words(Copy_tok['content'])))*100)\n",
    "#print(\"Reduction of UW from stopwords to after stemming (in %): \", (len(unique_words(Copy_stemming['content']))/len(unique_words(Copy_stop['content'])))*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_15184\\1582229614.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('df2_clean_tok_stop_stem.csv', usecols=['content', 'type', 'title'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumor</td>\n",
       "      <td>['life', 'illus', 'least', 'quantum', 'level',...</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>['unfortun', 'hasnt', 'yet', 'attack', 'islam'...</td>\n",
       "      <td>Donald Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>['los', 'angel', 'polic', 'depart', 'deni', 'n...</td>\n",
       "      <td>Donald Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>['white', 'hous', 'decid', 'quiet', 'withdraw'...</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>['time', 'come', 'cut', 'tongu', 'support', 'p...</td>\n",
       "      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999995</th>\n",
       "      <td>unknown</td>\n",
       "      <td>['emili', 'keeler', 'found', 'curat', 'million...</td>\n",
       "      <td>Emily Keeler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999996</th>\n",
       "      <td>unknown</td>\n",
       "      <td>['new', 'inquiri', 'new', 'inquiri', 'space', ...</td>\n",
       "      <td>Chloe Wyma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999997</th>\n",
       "      <td>bias</td>\n",
       "      <td>['eric', 'london', 'wide', 'rang', 'nomin', 'l...</td>\n",
       "      <td>Defend Democracy Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999998</th>\n",
       "      <td>unknown</td>\n",
       "      <td>['given', 'choic', 'white', 'black', 'could', ...</td>\n",
       "      <td>In a Mirror, Darkly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999999</th>\n",
       "      <td>unknown</td>\n",
       "      <td>['search', 'support', 'surveil', 'juli', 'prin...</td>\n",
       "      <td>Testing Beyond Control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                            content  \\\n",
       "0          rumor  ['life', 'illus', 'least', 'quantum', 'level',...   \n",
       "1           hate  ['unfortun', 'hasnt', 'yet', 'attack', 'islam'...   \n",
       "2           hate  ['los', 'angel', 'polic', 'depart', 'deni', 'n...   \n",
       "3           hate  ['white', 'hous', 'decid', 'quiet', 'withdraw'...   \n",
       "4           hate  ['time', 'come', 'cut', 'tongu', 'support', 'p...   \n",
       "...          ...                                                ...   \n",
       "1999995  unknown  ['emili', 'keeler', 'found', 'curat', 'million...   \n",
       "1999996  unknown  ['new', 'inquiri', 'new', 'inquiri', 'space', ...   \n",
       "1999997     bias  ['eric', 'london', 'wide', 'rang', 'nomin', 'l...   \n",
       "1999998  unknown  ['given', 'choic', 'white', 'black', 'could', ...   \n",
       "1999999  unknown  ['search', 'support', 'surveil', 'juli', 'prin...   \n",
       "\n",
       "                                                     title  \n",
       "0        Is life an ILLUSION? Researchers prove 'realit...  \n",
       "1                                             Donald Trump  \n",
       "2                                             Donald Trump  \n",
       "3        MORE WINNING! Israeli intelligence source, DEB...  \n",
       "4        “Oh, Trump, you coward, you just wait, we will...  \n",
       "...                                                    ...  \n",
       "1999995                                       Emily Keeler  \n",
       "1999996                                         Chloe Wyma  \n",
       "1999997                             Defend Democracy Press  \n",
       "1999998                                In a Mirror, Darkly  \n",
       "1999999                             Testing Beyond Control  \n",
       "\n",
       "[2000000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df2_clean_tok_stop_stem.csv', usecols=['content', 'type', 'title'])\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('id', inplace=True, axis=1)\n",
    "#df.drop('inserted_at', inplace=True, axis=1)\n",
    "#df.drop('scraped_at', inplace=True, axis=1)\n",
    "#df.drop('updated_at', inplace=True, axis=1)\n",
    "#df.drop('summary', inplace=True, axis=1)\n",
    "#df.drop('tags', inplace=True, axis=1)\n",
    "#df.drop('source', inplace=True, axis=1)\n",
    "\n",
    "# Remove articles with missing 'type' values\n",
    "df = df[df['type'].notna()]\n",
    "\n",
    "# Remove articles with 'type' values = 'unknown'\n",
    "df = df[df['type'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  196915\n"
     ]
    }
   ],
   "source": [
    "# Print number of duplicates\n",
    "print(\"Number of duplicates: \", df.duplicated(subset=['title', 'content']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of articles:  1551222\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate articles based on 'title' and 'content' columns\n",
    "df.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Print sum of articles for all types\n",
    "print(\"Total # of articles: \", df['type'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column 'label' and assign 1 to all rows with 'type' = 'fake' or 'conspiracy' or 'junksci' or 'hate' or 'unreliable' or 'rumor' and 0 to all other rows\n",
    "df['label'] = df['type'].apply(lambda x: 1 if x in ['fake', 'conspiracy', 'junksci', 'hate', 'unreliable', 'rumor'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240978, 4)\n",
      "(155122, 4)\n",
      "(155122, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "train_set = df.sample(frac=0.8, random_state=0)\n",
    "new_df2 = df.drop(train_set.index)\n",
    "validation_set = new_df2.sample(frac=0.5, random_state=0)\n",
    "test_set = new_df2.drop(validation_set.index)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(validation_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of unique words in content column\n",
    "def unique_words (file):\n",
    "    #return np.unique(file, return_counts=True)\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    #UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return len(UN)\n",
    "print(type(df['content'][0]))\n",
    "print(\"Number of unique words in content column: \", unique_words(df['content'][0]))\n",
    "print(len(df['content'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print content of first article in df\n",
    "print(df['content'][1])\n",
    "df['content'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique (file):\n",
    "    count = 0\n",
    "    for word in file:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Number of words in content 0: \", count_unique(df['content'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_15184\\2319731408.py\", line 4, in <module>\n",
      "    X_train = vectorizer.fit_transform(train_set['content'])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1387, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line -1, in _count_vocab\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "                                              ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\karen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training set\n",
    "#X_train = vectorizer.fit_transform(train_set['content'])\n",
    "#y_train = train_set['label']\n",
    "\n",
    "# Fit the vectorizer on the validation set\n",
    "#X_val = vectorizer.transform(validation_set['content'])\n",
    "#y_val = validation_set['label']\n",
    "\n",
    "# Fit the vectorizer on the test set\n",
    "#X_test = vectorizer.transform(test_set['content'])\n",
    "#y_test = test_set['label']\n",
    "\n",
    "#type(train_set['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940911     ['tor', 'tor', 'encrypt', 'anonymis', 'network...\n",
      "1614902    ['sen', 'graham', 'mccainthi', 'mind', 'specia...\n",
      "524327     ['yet', 'powersthatb', 'washington', 'wall', '...\n",
      "214355     ['meico', 'elect', 'pit', 'teacher', 'gang', '...\n",
      "1886794    ['brock', 'allen', 'turner', 'get', 'even', 's...\n",
      "                                 ...                        \n",
      "1493250    ['becom', 'first', 'sit', 'us', 'presid', 'att...\n",
      "1001369    ['ben', 'hutchison', 'given', 'mansfield', 'le...\n",
      "1507098    ['presid', 'donald', 'trump', 'made', 'case', ...\n",
      "1495381    ['presumpt', 'republican', 'nomine', 'donald',...\n",
      "47988      ['gather', 'number', 'peopl', 'drawn', 'villag...\n",
      "Name: content, Length: 1240978, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_set['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240978,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set['content'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use TF-IDF to transform the data\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit the transformer on the training set\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "y_train = train_set['label']\n",
    "\n",
    "# Fit the transformer on the validation set\n",
    "#X_val_tfidf = tfidf_transformer.transform(X_val)\n",
    "# y_val = validation_set['content']\n",
    "\n",
    "# Fit the transformer on the test set\n",
    "#X_test_tfidf = tfidf_transformer.transform(X_test)\n",
    "# y_test = test_set['label']\n",
    "X_train_tfidf = TfidfVectorizer(tokenizer=None)\n",
    "tfidf_matrix = X_train_tfidf.fit_transform(train_set['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240978, 1969872)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=TfidfVectorizer().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train a Naive Bayes classifier with tf-idf data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNB()\u001b[39m.\u001b[39;49mfit(X_train_tfidf, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Predict the labels for the validation set\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predicted \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_val_tfidf)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \n\u001b[0;32m    731\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 749\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:583\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    582\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:894\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    892\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 894\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    895\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    896\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    897\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    898\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    899\u001b[0m         )\n\u001b[0;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=TfidfVectorizer().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier with tf-idf data\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the validation set\n",
    "predicted = clf.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy_score(y_val, predicted)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, predicted))\n",
    "\n",
    "# Print the classification report\n",
    "classification_report(y_val, predicted)\n",
    "print(\"Classification report: \", classification_report(y_val, predicted))\n",
    "\n",
    "# Print number of features\n",
    "print(\"Number of features: \", clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 487130)\t0.08858741722494201\n",
      "  (0, 972426)\t0.08798661602748538\n",
      "  (0, 242189)\t0.06786518624582187\n",
      "  (0, 50686)\t0.08885044213547895\n",
      "  (0, 982885)\t0.06690437659219599\n",
      "  (0, 566072)\t0.07244601948288659\n",
      "  (0, 1491529)\t0.07088192583520694\n",
      "  (0, 1790945)\t0.08533985094072333\n",
      "  (0, 1524381)\t0.08492835729998947\n",
      "  (0, 193711)\t0.08575568344228908\n",
      "  (0, 1607383)\t0.08070151332023999\n",
      "  (0, 392985)\t0.16450616932844897\n",
      "  (0, 1789386)\t0.08638738463733635\n",
      "  (0, 983334)\t0.06864194850211712\n",
      "  (0, 99406)\t0.08030076397287096\n",
      "  (0, 540877)\t0.17678843471457514\n",
      "  (0, 1674797)\t0.16796868621101946\n",
      "  (0, 1847799)\t0.17734798518650685\n",
      "  (0, 976743)\t0.07794560220398504\n",
      "  (0, 305192)\t0.05542631501777232\n",
      "  (0, 1265032)\t0.14064463148483033\n",
      "  (0, 1572785)\t0.06597383604199615\n",
      "  (0, 71078)\t0.07652802114098482\n",
      "  (0, 122450)\t0.04714733358798288\n",
      "  (0, 312917)\t0.08389622939067806\n",
      "  :\t:\n",
      "  (9, 1258793)\t0.01471680456187684\n",
      "  (9, 1530618)\t0.024261361556769456\n",
      "  (9, 990666)\t0.02539411286761605\n",
      "  (9, 1210770)\t0.022009747986412857\n",
      "  (9, 128930)\t0.04054003659435072\n",
      "  (9, 1605246)\t0.04144795413911545\n",
      "  (9, 980874)\t0.02461211050005831\n",
      "  (9, 1492296)\t0.02748297420704827\n",
      "  (9, 783000)\t0.06254636827018466\n",
      "  (9, 1666228)\t0.022527545346528483\n",
      "  (9, 1744789)\t0.02415920241886384\n",
      "  (9, 71851)\t0.020926865978021708\n",
      "  (9, 1767798)\t0.03232890320153167\n",
      "  (9, 78342)\t0.02451051171738349\n",
      "  (9, 1507614)\t0.01915200042322777\n",
      "  (9, 1235868)\t0.12034237843114062\n",
      "  (9, 120658)\t0.02414696185949454\n",
      "  (9, 765396)\t0.023493494604779088\n",
      "  (9, 910315)\t0.02743269052156544\n",
      "  (9, 1581810)\t0.07167623871986097\n",
      "  (9, 392985)\t0.025254551175017764\n",
      "  (9, 1849087)\t0.015712132618539983\n",
      "  (9, 383485)\t0.016361377174570747\n",
      "  (9, 767018)\t0.026206053741462634\n",
      "  (9, 1040992)\t0.06059392488775872\n",
      "(1240978, 1969872)\n"
     ]
    }
   ],
   "source": [
    "#print the first 10 values in tfidf_matrix\n",
    "print(tfidf_matrix[0:10])\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try SVM model on the training set with TF-IDF data\n",
    "\n",
    "svm = LinearSVC(random_state=0).fit(tfidf_matrix, y_train)\n",
    "\n",
    "# Predict the labels for the validation set\n",
    "#predicted = svm.predict(X_val_tfidf)\n",
    "\n",
    "# Print number of features\n",
    "#print(\"Number of features: \", svm.n_features_in_)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "#accuracy_score(y_val, predicted)\n",
    "#print(\"Accuracy: \", accuracy_score(y_val, predicted))\n",
    "\n",
    "# Print the classification report\n",
    "#classification_report(y_val, predicted)\n",
    "#print(\"Classification report: \", classification_report(y_val, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "LogRegFinal = LogisticRegression(random_state=0, solver='lbfgs', max_iter=2000)\n",
    "\n",
    "# Fit the model on the training set\n",
    "LogRegFinal.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the validation set\n",
    "y_pred = LogRegFinal.predict(X_val)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy score: \", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# Print the classification report\n",
    "classification_report(y_val, y_pred, output_dict=True)\n",
    "print(\"Classification report: \", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set (SVM)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the classification report\n",
    "classification_report(y_test, y_pred)\n",
    "print(\"Classification report: \", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for test set (SVM)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           true\n",
      "1          false\n",
      "2          false\n",
      "3      half-true\n",
      "4     pants-fire\n",
      "5           true\n",
      "6           true\n",
      "7    barely-true\n",
      "8           true\n",
      "9    barely-true\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load test.tsv file\n",
    "\n",
    "dftsv = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "dftsv.head()\n",
    "\n",
    "# # # Print statement column for first article\n",
    "#print(dftsv[2][0])\n",
    "# print colomn 1 for first 10 articles\n",
    "print(dftsv[1][0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "5    0\n",
      "6    0\n",
      "7    1\n",
      "8    0\n",
      "9    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# new colomn for the label of the article\n",
    "dftsv['label'] = dftsv[1].apply(lambda x: 1 if x in ['false', 'pants-fire', 'barely-true'] else 0)\n",
    "print(dftsv['label'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanin the column 2 of test.tsv file (content) with cleaningTime function\n",
    "# convert dftsv[2] to string\n",
    "dftsv[2] = dftsv[2].astype(str)\n",
    "\n",
    "dftsv[2] = dftsv[2].apply(cleaningTime)\n",
    "dftsv[2] = dftsv[2].apply(Tokenize)\n",
    "dftsv[2] = dftsv[2].apply(Remove_stopwords)\n",
    "dftsv[2] = dftsv[2].apply(Stemming_Words)\n",
    "\n",
    "#print the shape of the new dftsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267,)\n"
     ]
    }
   ],
   "source": [
    "print(dftsv[2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       ['build', 'wall', 'usmexico', 'border', 'take'...\n",
      "1       ['wisconsin', 'pace', 'doubl', 'number', 'layo...\n",
      "2       ['say', 'john', 'mccain', 'done', 'noth', 'hel...\n",
      "3       ['suzann', 'bonamici', 'support', 'plan', 'cut...\n",
      "4       ['ask', 'report', 'whether', 'hes', 'center', ...\n",
      "                              ...                        \n",
      "1262    ['say', 'budget', 'provid', 'highest', 'state'...\n",
      "1263                    ['ive', 'almost', 'everi', 'day']\n",
      "1264    ['ear', '1980s', 'sen', 'edward', 'kennedi', '...\n",
      "1265    ['say', 'epa', 'permit', 'languish', 'strickla...\n",
      "1266    ['say', 'governor', 'go', 'around', 'state', '...\n",
      "Name: 2, Length: 1267, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import TfidfVectorizer\n",
    "liar_tfidf_matrix = X_train_tfidf.transform(dftsv[2].astype(str))\n",
    "print(dftsv[2].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267, 1969872)\n"
     ]
    }
   ],
   "source": [
    "print(liar_tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3138)\t0.21259570585299392\n",
      "  (0, 1629)\t0.42256852606565126\n",
      "  (0, 2778)\t0.32765069078192804\n",
      "  (0, 338)\t0.37646714852082286\n",
      "  (0, 2994)\t0.46866990361047967\n",
      "  (0, 3049)\t0.39560094897033826\n",
      "  (0, 377)\t0.3885933969714461\n",
      "  (1, 1579)\t0.5501671204976506\n",
      "  (1, 1931)\t0.14357163138878662\n",
      "  (1, 834)\t0.4478202919297405\n",
      "  (1, 2014)\t0.5501671204976506\n",
      "  (1, 3108)\t0.34320166470012897\n",
      "  (1, 3138)\t0.23598531562373068\n",
      "  (2, 3011)\t0.4980273897992678\n",
      "  (2, 1293)\t0.3675709489567165\n",
      "  (2, 1920)\t0.41293489019889473\n",
      "  (2, 829)\t0.4061987069497626\n",
      "  (2, 1733)\t0.35105988528536314\n",
      "  (2, 1482)\t0.3604974074060566\n",
      "  (2, 2481)\t0.1677685298811845\n",
      "  (3, 2524)\t0.3201782965824566\n",
      "  (3, 65)\t0.369699807764814\n",
      "  (3, 1747)\t0.2858917941708913\n",
      "  (3, 505)\t0.3823220812969076\n",
      "  (3, 691)\t0.24059522639927117\n",
      "  :\t:\n",
      "  (7, 126)\t0.1988645618746472\n",
      "  (7, 61)\t0.3398033054095819\n",
      "  (7, 298)\t0.3542665011175018\n",
      "  (7, 1481)\t0.3194185759708568\n",
      "  (7, 2181)\t0.18856853736139134\n",
      "  (7, 3015)\t0.3746512305562269\n",
      "  (7, 2481)\t0.11934036419681605\n",
      "  (8, 227)\t0.3887400418189412\n",
      "  (8, 1180)\t0.3055781587385502\n",
      "  (8, 3052)\t0.3261465860412457\n",
      "  (8, 928)\t0.4432308278502002\n",
      "  (8, 1711)\t0.45425215088457\n",
      "  (8, 2923)\t0.3467150133439412\n",
      "  (8, 826)\t0.3551361561205568\n",
      "  (9, 1097)\t0.2961880141452948\n",
      "  (9, 621)\t0.3638801318404756\n",
      "  (9, 1681)\t0.26886223194244135\n",
      "  (9, 1176)\t0.3102353977698433\n",
      "  (9, 2678)\t0.14930119555794644\n",
      "  (9, 2511)\t0.30270829079003175\n",
      "  (9, 1749)\t0.3102353977698433\n",
      "  (9, 540)\t0.4539866609476417\n",
      "  (9, 1311)\t0.2446214465226392\n",
      "  (9, 1536)\t0.2763893389222529\n",
      "  (9, 1236)\t0.2425432800746625\n",
      "(1267, 3150)\n"
     ]
    }
   ],
   "source": [
    "print(liar_tfidf_matrix[0:10])\n",
    "#print the shape of the tfidf_matrix\n",
    "print(liar_tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(liar_tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5493291239147593\n"
     ]
    }
   ],
   "source": [
    "#print the accuracy score\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(dftsv['label'], predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
