{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install clean-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sometimes, the, power, of, christmas, will, m...\n",
      "1      [awakening, of, number, strands, of, dna, reco...\n",
      "2      [never, hike, alone, a, friday, the, 13th, fan...\n",
      "3      [when, a, rare, shark, was, caught, scientists...\n",
      "4      [donald, trump, has, the, unnerving, ability, ...\n",
      "                             ...                        \n",
      "245    [prison, for, rahm, gods, work, and, many, oth...\n",
      "246    [number, useful, items, for, your, tiny, home,...\n",
      "247    [former, cia, director, michael, hayden, said,...\n",
      "248    [antonio, sabato, jr, says, hollywoods, libera...\n",
      "249    [former, us, president, bill, clinton, on, mon...\n",
      "Name: content, Length: 250, dtype: object\n",
      "Stop words length:  179\n",
      "Stop words dictionary:  {'t', 'be', 'below', 'been', 'shouldn', 'myself', 'the', 'most', \"aren't\", 'when', \"didn't\", 'ours', 'under', 'into', 'whom', 'd', 'there', 'doesn', 'more', 'or', 'as', 'between', 'very', \"should've\", \"couldn't\", 'his', 'y', 'we', 'until', 'this', \"needn't\", \"wasn't\", 'doing', \"you'll\", 'because', 'haven', 'to', 'have', 'should', 'll', 'aren', 'off', 'other', 'so', 'only', 'will', 'hadn', 'hers', 'for', \"it's\", 'both', 'do', 'had', 'your', 'after', 'then', 'out', 'isn', 'himself', \"that'll\", 'from', 'too', 'at', 'ma', 'was', 'ourselves', 'wouldn', 'who', 'nor', 'own', 'down', \"mightn't\", 'mustn', 'some', 'about', 'not', 'o', 'theirs', 'they', 'you', \"don't\", 'that', 'how', 'any', 'did', 'weren', 'has', 'just', 'what', 'ain', 'by', 'again', 'no', 'herself', 'its', 'she', 'all', \"doesn't\", 'does', 'further', 'him', 'me', 'where', 've', 're', 'which', 'couldn', 'won', 'being', 'but', 'mightn', \"shouldn't\", 'such', \"wouldn't\", 'above', 'can', 'their', 'were', 'it', \"haven't\", 'if', 'having', 'our', 'is', 'in', 'yourself', 's', \"shan't\", \"you're\", 'few', 'didn', 'i', \"you'd\", 'm', \"hasn't\", 'my', \"mustn't\", 'shan', 'why', 'these', \"isn't\", 'a', 'on', \"hadn't\", 'over', 'yours', 'through', 'same', \"weren't\", \"won't\", 'themselves', 'and', 'don', 'an', 'once', \"you've\", 'than', 'during', 'yourselves', 'her', 'those', 'needn', 'itself', 'them', 'while', \"she's\", 'with', 'each', 'before', 'now', 'wasn', 'here', 'up', 'are', 'against', 'he', 'am', 'of', 'hasn'}\n",
      "Difference between number of tokens before and after removing stopwords:  133\n",
      "Number of words after tokenizing:  16641\n",
      "Number of words after removing stopwords:  16508\n",
      "Reduction rate after removing stopwords:  0.7992308154558003\n",
      "Number of words after stemming:  10998\n",
      "Reduction rate after stemming:  33.377756239399076\n"
     ]
    }
   ],
   "source": [
    "#################################### PACKAGES ######################################\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "R = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\") \n",
    "\n",
    "\n",
    "####################################### CLEANING ######################################\n",
    "Copy_clean = R.copy()\n",
    "\n",
    "def cleaningTime(x):\n",
    "    if isinstance(x, str):        \n",
    "        clean_text = x.replace('|', '')\n",
    "        clean_text = clean_text.replace('$', '')\n",
    "        regex = r'[xX]+'\n",
    "        clean_text = re.sub(regex, '', clean_text)\n",
    "        # Replace date with DATE\n",
    "        regex = r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}'\n",
    "        clean_text = re.sub(regex, 'DATE', clean_text)\n",
    "        clean_text = clean(clean_text,\n",
    "            no_punct=True,\n",
    "            lower=True,                    # lowercase text\n",
    "            no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "            no_urls=True,                  # replace all URLs with a special token\n",
    "            no_emails=True,                # replace all email addresses with a special token\n",
    "            no_numbers=True,\n",
    "            replace_with_url=\"url\",\n",
    "            replace_with_email=\"email\",\n",
    "            replace_with_number=\"number\",\n",
    "            )\n",
    "        return clean_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#R['content'].apply(cleaning)\n",
    "Copy_clean['content'] = R['content'].apply(cleaningTime)\n",
    "#print(Copy_clean['content'])\n",
    "\n",
    "# (r'C:\\Users\\karen\\nein.csv', index= None, header=True)\n",
    "df = pd.DataFrame(Copy_clean)\n",
    "df.to_csv('copy_clean.csv', index= None, header=True)\n",
    "\n",
    "\n",
    "####################################### TOKENIZE #########################################\n",
    "Copy_tok = Copy_clean.copy()\n",
    "\n",
    "def Tokenize(x):     \n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "Copy_tok['content'] = Copy_tok['content'].apply(Tokenize)\n",
    "print(Copy_tok['content'])\n",
    "\n",
    "#Copy_stop = []\n",
    "#for i in Copy_tok['content']:\n",
    "    #tokens = nltk.word_tokenize(i)\n",
    "    #Copy_stop.append(tokens)\n",
    "#print(Copy_stop)\n",
    "\n",
    "######################################## REMOVING STOPWORDS ################################\n",
    "Copy_stop = Copy_tok.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop words length: \", len(stop_words))\n",
    "print(\"Stop words dictionary: \", (stop_words))\n",
    "# removing\n",
    "def Remove_stopwords(x): # x = Copy_tok['content']\n",
    "    filtered_sentence = []\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "Copy_stop['content'] = Copy_stop['content'].apply(Remove_stopwords)\n",
    "#print(Copy_stop['content'])\n",
    "\n",
    "\n",
    "######################################### STEMMING ##########################################\n",
    "Copy_stemming = Copy_stop.copy()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def Stemming_Words(x):\n",
    "    stem_words = []\n",
    "    for w in x:\n",
    "        word = stemmer.stem(w)\n",
    "        stem_words.append(word)\n",
    "    return stem_words\n",
    "Copy_stemming['content'] = Copy_stemming['content'].apply(Stemming_Words)\n",
    "#print(Copy_stemming['content'])\n",
    "\n",
    "##############################################################################################\n",
    "# number of tokens before removing stopwords\n",
    "def unique_words (file):\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return len(UN)\n",
    "\n",
    "\n",
    "# Difference between number of tokens before and after removing stopwords\n",
    "print(\"Difference between number of tokens before and after removing stopwords: \", (unique_words(Copy_tok['content'])-unique_words(Copy_stop['content'])))\n",
    "\n",
    "# number of tokens after tokenizing\n",
    "print(\"Number of words after tokenizing: \", unique_words(Copy_tok['content']))\n",
    "\n",
    "# number of tokens after removing stopwords\n",
    "print(\"Number of words after removing stopwords: \", unique_words(Copy_stop['content']))\n",
    "\n",
    "# reduction rate after removing stopwords\n",
    "print(\"Reduction rate after removing stopwords: \", 100 - unique_words(Copy_stop['content'])/unique_words(Copy_tok['content']) * 100)\n",
    "\n",
    "# number of tokens after stemming\n",
    "print(\"Number of words after stemming: \", unique_words(Copy_stemming['content']))\n",
    "\n",
    "# reduction rate after stemming\n",
    "print(\"Reduction rate after stemming: \", 100 - unique_words(Copy_stemming['content'])/unique_words(Copy_stop['content']) * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The URLs are not correctly formatted: Each URL includes the first word in the content!!!\n",
    "* Not sure whether the counts below are done in the right order\n",
    "* Regarding dates: We need to choose which column to use: (1) scraped_at, (2) inserted_at, (3) updated_at\n",
    "* Insert more \"replace\" statements in CleaningTime (i.e. DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs:  242\n",
      "Number of dates:  27\n",
      "Number of numeric values:  2019\n",
      "Number of punctuations (check):  0\n"
     ]
    }
   ],
   "source": [
    "# Count URLs\n",
    "count_url = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'url':\n",
    "            count_url += 1\n",
    "        else:\n",
    "            count_url = count_url\n",
    "print(\"Number of URLs: \", count_url)\n",
    "\n",
    "# Count dates\n",
    "count_date = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'date':\n",
    "            count_date += 1\n",
    "        else:\n",
    "            count_date = count_date\n",
    "print(\"Number of dates: \", count_date)\n",
    "\n",
    "# Count numbers (numeric values)\n",
    "count_number = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'number':\n",
    "            count_number += 1\n",
    "        else:\n",
    "            count_number = count_number\n",
    "print(\"Number of numeric values: \", count_number)\n",
    "\n",
    "# Count punctuation: check if punctuations are removed\n",
    "count_punct = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'punct':\n",
    "            count_punct += 1\n",
    "        else:\n",
    "            count_punct = count_punct\n",
    "print(\"Number of punctuations (check): \", count_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_4072\\3179769783.py:7: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ = pd.read_csv('1mio-raw.csv')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "#csv = open('news_cleaned_2018_02_13.csv')\n",
    "#csv_ = open('1mio-raw.csv')\n",
    "#df = pd.read_csv('news_cleaned_2018_02_13.csv')\n",
    "#df = pd.read_csv('news_cleaned_2018_02_13.csv',sep=',', header = None, skiprows=1000, chunksize=10)\n",
    "df_ = pd.read_csv('1mio-raw.csv')\n",
    "\n",
    "# apply cleaningtime function to the content column on a copy of the dataframe\n",
    "# make deep copy of the dataframe\n",
    "# df_copy = df_.copy(deep=True)\n",
    "# apply cleaningtime function to the content column. Handle attrribute error float has no attribute replace\n",
    "# for x in df_copy['content'][:100]:\n",
    "#     try:\n",
    "#         df_copy['content'][:100] = df_copy['content'][:100].apply(cleaningTime)\n",
    "#     except AttributeError:\n",
    "#         pass\n",
    "\n",
    "# print(df_copy.head(100)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      life is an illusion at least on a quantum leve...\n",
      "1      unfortunately he hasnt yet attacked her for is...\n",
      "2      the los angeles police department has been den...\n",
      "3      the white house has decided to quietly withdra...\n",
      "4      the time has come to cut off the tongues of th...\n",
      "                             ...                        \n",
      "97     martina big a numberyearold white german model...\n",
      "98     sushi is a trendy dish it comes from japan and...\n",
      "99     a numberyearold british vlogger has made a nam...\n",
      "100    (Facebook/Support Kountze Kids Faith)\\n\\nA Tex...\n",
      "101    Concentration Camps for African Migrants Block...\n",
      "Name: content, Length: 102, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_copy.head(102)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/news/science/738402/...</td>\n",
       "      <td>Life is an illusion, at least on a quantum lev...</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>The Los Angeles Police Department has been den...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n",
       "      <td>The White House has decided to quietly withdra...</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n",
       "      <td>“The time has come to cut off the tongues of t...</td>\n",
       "      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n",
       "      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>9929</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976IBA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976IBADAN00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>9930</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976BOM...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976BOMBAY00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>9931</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>9932</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974BAN...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1974BANGKO15999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>9933</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1974STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              domain        type  \\\n",
       "0               0       express.co.uk       rumor   \n",
       "1               1  barenakedislam.com        hate   \n",
       "2               2  barenakedislam.com        hate   \n",
       "3               3  barenakedislam.com        hate   \n",
       "4               4  barenakedislam.com        hate   \n",
       "...           ...                 ...         ...   \n",
       "999995       9929       wikileaks.org  unreliable   \n",
       "999996       9930       wikileaks.org  unreliable   \n",
       "999997       9931       wikileaks.org  unreliable   \n",
       "999998       9932       wikileaks.org  unreliable   \n",
       "999999       9933       wikileaks.org  unreliable   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://www.express.co.uk/news/science/738402/...   \n",
       "1       http://barenakedislam.com/category/donald-trum...   \n",
       "2       http://barenakedislam.com/category/donald-trum...   \n",
       "3       http://barenakedislam.com/2017/12/24/more-winn...   \n",
       "4       http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
       "...                                                   ...   \n",
       "999995  https://www.wikileaks.org/plusd/cables/1976IBA...   \n",
       "999996  https://www.wikileaks.org/plusd/cables/1976BOM...   \n",
       "999997  https://www.wikileaks.org/plusd/cables/1976STA...   \n",
       "999998  https://www.wikileaks.org/plusd/cables/1974BAN...   \n",
       "999999  https://www.wikileaks.org/plusd/cables/1974STA...   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Life is an illusion, at least on a quantum lev...   \n",
       "1       Unfortunately, he hasn’t yet attacked her for ...   \n",
       "2       The Los Angeles Police Department has been den...   \n",
       "3       The White House has decided to quietly withdra...   \n",
       "4       “The time has come to cut off the tongues of t...   \n",
       "...                                                   ...   \n",
       "999995  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999996  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999997  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999998  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999999  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                            Donald Trump   \n",
       "2                                            Donald Trump   \n",
       "3       MORE WINNING! Israeli intelligence source, DEB...   \n",
       "4       “Oh, Trump, you coward, you just wait, we will...   \n",
       "...                                                   ...   \n",
       "999995                             Cable: 1976IBADAN00030   \n",
       "999996                             Cable: 1976BOMBAY00030   \n",
       "999997                             Cable: 1976STATE087319   \n",
       "999998                             Cable: 1974BANGKO15999   \n",
       "999999                             Cable: 1974STATE087319   \n",
       "\n",
       "                                                  authors  keywords  \\\n",
       "0                                             Sean Martin       NaN   \n",
       "1       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "2       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "3       Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN   \n",
       "4       F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN   \n",
       "...                                                   ...       ...   \n",
       "999995                                                NaN       NaN   \n",
       "999996                                                NaN       NaN   \n",
       "999997                                                NaN       NaN   \n",
       "999998                                                NaN       NaN   \n",
       "999999                                                NaN       NaN   \n",
       "\n",
       "       meta_keywords                                   meta_description  \n",
       "0               ['']  THE UNIVERSE ceases to exist when we are not l...  \n",
       "1               ['']                                                NaN  \n",
       "2               ['']                                                NaN  \n",
       "3               ['']                                                NaN  \n",
       "4               ['']                                                NaN  \n",
       "...              ...                                                ...  \n",
       "999995          ['']                                                NaN  \n",
       "999996          ['']                                                NaN  \n",
       "999997          ['']                                                NaN  \n",
       "999998          ['']                                                NaN  \n",
       "999999          ['']                                                NaN  \n",
       "\n",
       "[1000000 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.drop('id', inplace=True, axis=1)\n",
    "df_.drop('inserted_at', inplace=True, axis=1)\n",
    "df_.drop('scraped_at', inplace=True, axis=1)\n",
    "df_.drop('updated_at', inplace=True, axis=1)\n",
    "df_.drop('summary', inplace=True, axis=1)\n",
    "df_.drop('tags', inplace=True, axis=1)\n",
    "df_.drop('source', inplace=True, axis=1)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/news/science/738402/...</td>\n",
       "      <td>Life is an illusion, at least on a quantum lev...</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>The Los Angeles Police Department has been den...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n",
       "      <td>The White House has decided to quietly withdra...</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n",
       "      <td>“The time has come to cut off the tongues of t...</td>\n",
       "      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n",
       "      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>9929</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976IBA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976IBADAN00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>9930</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976BOM...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976BOMBAY00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>9931</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1976STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>9932</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974BAN...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1974BANGKO15999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>9933</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>Cable: 1974STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              domain        type  \\\n",
       "0               0       express.co.uk       rumor   \n",
       "1               1  barenakedislam.com        hate   \n",
       "2               2  barenakedislam.com        hate   \n",
       "3               3  barenakedislam.com        hate   \n",
       "4               4  barenakedislam.com        hate   \n",
       "...           ...                 ...         ...   \n",
       "999995       9929       wikileaks.org  unreliable   \n",
       "999996       9930       wikileaks.org  unreliable   \n",
       "999997       9931       wikileaks.org  unreliable   \n",
       "999998       9932       wikileaks.org  unreliable   \n",
       "999999       9933       wikileaks.org  unreliable   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://www.express.co.uk/news/science/738402/...   \n",
       "1       http://barenakedislam.com/category/donald-trum...   \n",
       "2       http://barenakedislam.com/category/donald-trum...   \n",
       "3       http://barenakedislam.com/2017/12/24/more-winn...   \n",
       "4       http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
       "...                                                   ...   \n",
       "999995  https://www.wikileaks.org/plusd/cables/1976IBA...   \n",
       "999996  https://www.wikileaks.org/plusd/cables/1976BOM...   \n",
       "999997  https://www.wikileaks.org/plusd/cables/1976STA...   \n",
       "999998  https://www.wikileaks.org/plusd/cables/1974BAN...   \n",
       "999999  https://www.wikileaks.org/plusd/cables/1974STA...   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Life is an illusion, at least on a quantum lev...   \n",
       "1       Unfortunately, he hasn’t yet attacked her for ...   \n",
       "2       The Los Angeles Police Department has been den...   \n",
       "3       The White House has decided to quietly withdra...   \n",
       "4       “The time has come to cut off the tongues of t...   \n",
       "...                                                   ...   \n",
       "999995  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999996  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999997  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999998  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999999  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                            Donald Trump   \n",
       "2                                            Donald Trump   \n",
       "3       MORE WINNING! Israeli intelligence source, DEB...   \n",
       "4       “Oh, Trump, you coward, you just wait, we will...   \n",
       "...                                                   ...   \n",
       "999995                             Cable: 1976IBADAN00030   \n",
       "999996                             Cable: 1976BOMBAY00030   \n",
       "999997                             Cable: 1976STATE087319   \n",
       "999998                             Cable: 1974BANGKO15999   \n",
       "999999                             Cable: 1974STATE087319   \n",
       "\n",
       "                                                  authors  keywords  \\\n",
       "0                                             Sean Martin       NaN   \n",
       "1       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "2       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "3       Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN   \n",
       "4       F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN   \n",
       "...                                                   ...       ...   \n",
       "999995                                                NaN       NaN   \n",
       "999996                                                NaN       NaN   \n",
       "999997                                                NaN       NaN   \n",
       "999998                                                NaN       NaN   \n",
       "999999                                                NaN       NaN   \n",
       "\n",
       "       meta_keywords                                   meta_description  label  \n",
       "0               ['']  THE UNIVERSE ceases to exist when we are not l...      1  \n",
       "1               ['']                                                NaN      1  \n",
       "2               ['']                                                NaN      1  \n",
       "3               ['']                                                NaN      1  \n",
       "4               ['']                                                NaN      1  \n",
       "...              ...                                                ...    ...  \n",
       "999995          ['']                                                NaN      1  \n",
       "999996          ['']                                                NaN      1  \n",
       "999997          ['']                                                NaN      1  \n",
       "999998          ['']                                                NaN      1  \n",
       "999999          ['']                                                NaN      1  \n",
       "\n",
       "[1000000 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new column 'label' and assign 1 to all rows with 'type' = 'fake' or 'conspiracy' or 'bias' or 'junksci' or 'hate' or 'unreliable' or 'rumor' and 0 to all other rows\n",
    "df_['label'] = df_['type'].apply(lambda x: 1 if x == 'fake' or x == 'conspiracy' or x == 'bias' or x == 'junksci' or x == 'hate' or x == 'unreliable' or x == 'rumor' else 0)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake 579268\n",
      "Real 420732\n"
     ]
    }
   ],
   "source": [
    "df_.tail(20)\n",
    "count = 0\n",
    "for i in df_['label']:\n",
    "    if i == 1:\n",
    "        count += 1\n",
    "\n",
    "print(\"Fake\", count)\n",
    "\n",
    "df_.tail(20)\n",
    "count = 0\n",
    "for i in df_['label']:\n",
    "    if i == 0:\n",
    "        count += 1\n",
    "\n",
    "print(\"Real\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "# cleaning the content column\n",
    "df_['content'] = df_['content'].apply(cleaningTime)\n",
    "# apply tokenization function to the content column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cleaned data in a new csv file\n",
    "df_.to_csv('df_cleaned_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 11)\n",
      "(100000, 11)\n",
      "(100000, 11)\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset into train, validation and test set\n",
    "train_set = df_.sample(frac=0.8, random_state=0)\n",
    "new_df = df_.drop(train_set.index)\n",
    "validation_set = new_df.sample(frac=0.5, random_state=0)\n",
    "test_set = new_df.drop(validation_set.index)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(validation_set.shape)\n",
    "print(test_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              25\n",
       "domain                  52\n",
       "type                 32722\n",
       "url                     52\n",
       "content                  0\n",
       "title                 7728\n",
       "authors             366000\n",
       "keywords            800000\n",
       "meta_keywords           52\n",
       "meta_description    564005\n",
       "label                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the train set\n",
    "train_set.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label articles with missing values in type column as fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m      9\u001b[0m \u001b[39m# tokenize and build vocab\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m vectorizer\u001b[39m.\u001b[39;49mfit(train_set[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, raw_documents, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m     \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[39m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n\u001b[0;32m   1277\u001b[0m \u001b[39mif\u001b[39;00m feature_idx \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m feature_counter:\n\u001b[1;32m-> 1278\u001b[0m     feature_counter[feature_idx] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     feature_counter[feature_idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# baseline model using count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Remove rows with missing values in the content column\n",
    "\n",
    "\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(train_set['content'])\n",
    "# summarize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "900b9bb925fcdb02ec6c2453d36a87d904f6c7d996c9903f56d484ca6ecf1666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
