{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install clean-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [sometimes, the, power, of, christmas, will, m...\n",
      "1      [awakening, of, number, strands, of, dna, reco...\n",
      "2      [never, hike, alone, a, friday, the, 13th, fan...\n",
      "3      [when, a, rare, shark, was, caught, scientists...\n",
      "4      [donald, trump, has, the, unnerving, ability, ...\n",
      "                             ...                        \n",
      "245    [prison, for, rahm, gods, work, and, many, oth...\n",
      "246    [number, useful, items, for, your, tiny, home,...\n",
      "247    [former, cia, director, michael, hayden, said,...\n",
      "248    [antonio, sabato, jr, says, hollywoods, libera...\n",
      "249    [former, us, president, bill, clinton, on, mon...\n",
      "Name: content, Length: 250, dtype: object\n",
      "Stop words length:  179\n",
      "Stop words dictionary:  {'into', 'no', \"won't\", 'him', 'an', 'under', 'through', 'll', \"should've\", 'other', 'above', 'will', \"mustn't\", 'doing', 'during', \"couldn't\", 'had', 'i', 'hers', 'why', 'just', 'but', 'is', \"it's\", \"that'll\", 'on', 'she', 'was', 'they', 'herself', 'has', 'himself', 'very', 's', 'mightn', 'each', 'that', 'until', 'what', 're', 'can', 'ours', 'being', 'out', 'weren', 'didn', 'y', \"mightn't\", 'd', 'their', 'couldn', 'with', 'further', 'did', 'were', 'myself', 'between', 'our', 'them', 'because', 'below', 'who', 'only', 'so', 'while', \"you've\", 'which', \"don't\", 'your', 've', 't', 'in', 'up', 'most', 'than', 'we', \"didn't\", 'her', 'of', \"shouldn't\", 'after', 'against', 'he', 'having', 'all', 'itself', 'the', 'such', 'don', 'does', \"haven't\", 'ma', 'if', 'm', 'this', 'have', 'needn', 'won', 'are', 'as', 'been', 'hadn', 'by', 'doesn', 'and', 'same', 'from', \"wouldn't\", 'some', 'shan', 'wasn', 'my', \"you're\", 'yourselves', 'theirs', 'whom', 'these', 'wouldn', 'ourselves', 'mustn', 'at', 'own', 'when', 'again', 'too', 'shouldn', 'you', 'am', 'those', 'few', 'there', 'nor', 'yours', 'any', 'its', 'me', 'once', 'ain', 'yourself', 'where', 'or', 'to', 'isn', 'here', 'hasn', 'his', 'down', 'more', 'over', \"isn't\", 'for', 'about', 'now', 'off', \"doesn't\", \"hadn't\", \"you'd\", \"weren't\", \"wasn't\", \"shan't\", \"you'll\", 'how', 'before', \"hasn't\", 'both', 'a', \"needn't\", 'o', 'should', 'do', 'aren', 'be', 'it', \"aren't\", 'themselves', 'then', 'not', \"she's\", 'haven'}\n",
      "Difference between number of tokens before and after removing stopwords:  133\n",
      "Number of words after tokenizing:  16641\n",
      "Number of words after removing stopwords:  16508\n",
      "Reduction rate after removing stopwords:  0.7992308154558003\n",
      "Number of words after stemming:  10998\n",
      "Reduction rate after stemming:  33.377756239399076\n"
     ]
    }
   ],
   "source": [
    "#################################### PACKAGES ######################################\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "R = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\") \n",
    "\n",
    "\n",
    "####################################### CLEANING ######################################\n",
    "Copy_clean = R.copy()\n",
    "\n",
    "def cleaningTime(x):\n",
    "    clean_text = x.replace('|', '')\n",
    "    clean_text = clean_text.replace('$', '')\n",
    "    regex = r'[xX]+'\n",
    "    clean_text = re.sub(regex, '', clean_text)\n",
    "    # Replace date with DATE\n",
    "    regex = r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}'\n",
    "    clean_text = re.sub(regex, 'DATE', clean_text)\n",
    "    clean_text = clean(clean_text,\n",
    "        no_punct=True,\n",
    "        lower=True,                    # lowercase text\n",
    "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,\n",
    "        replace_with_url=\"url\",\n",
    "        replace_with_email=\"email\",\n",
    "        replace_with_number=\"number\",\n",
    "        )\n",
    "    return clean_text\n",
    "\n",
    "#R['content'].apply(cleaning)\n",
    "Copy_clean['content'] = R['content'].apply(cleaningTime)\n",
    "#print(Copy_clean['content'])\n",
    "\n",
    "# (r'C:\\Users\\karen\\nein.csv', index= None, header=True)\n",
    "df = pd.DataFrame(Copy_clean)\n",
    "df.to_csv('copy_clean.csv', index= None, header=True)\n",
    "\n",
    "\n",
    "####################################### TOKENIZE #########################################\n",
    "Copy_tok = Copy_clean.copy()\n",
    "\n",
    "def Tokenize(x):     \n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "Copy_tok['content'] = Copy_tok['content'].apply(Tokenize)\n",
    "print(Copy_tok['content'])\n",
    "\n",
    "#Copy_stop = []\n",
    "#for i in Copy_tok['content']:\n",
    "    #tokens = nltk.word_tokenize(i)\n",
    "    #Copy_stop.append(tokens)\n",
    "#print(Copy_stop)\n",
    "\n",
    "######################################## REMOVING STOPWORDS ################################\n",
    "Copy_stop = Copy_tok.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop words length: \", len(stop_words))\n",
    "print(\"Stop words dictionary: \", (stop_words))\n",
    "# removing\n",
    "def Remove_stopwords(x): # x = Copy_tok['content']\n",
    "    filtered_sentence = []\n",
    "    for w in x:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "Copy_stop['content'] = Copy_stop['content'].apply(Remove_stopwords)\n",
    "#print(Copy_stop['content'])\n",
    "\n",
    "\n",
    "######################################### STEMMING ##########################################\n",
    "Copy_stemming = Copy_stop.copy()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def Stemming_Words(x):\n",
    "    stem_words = []\n",
    "    for w in x:\n",
    "        word = stemmer.stem(w)\n",
    "        stem_words.append(word)\n",
    "    return stem_words\n",
    "Copy_stemming['content'] = Copy_stemming['content'].apply(Stemming_Words)\n",
    "#print(Copy_stemming['content'])\n",
    "\n",
    "##############################################################################################\n",
    "# number of tokens before removing stopwords\n",
    "def unique_words (file):\n",
    "    UN = {}\n",
    "    for row in file:\n",
    "        for word in row:\n",
    "            if word not in UN:\n",
    "                UN[word] = 1\n",
    "            else: \n",
    "                UN[word] += 1\n",
    "    UN = dict(sorted(UN.items(), key = lambda kv: kv[1], reverse=True)) # Sorting the unique words after number of occurrences, from highest to lowest\n",
    "    return len(UN)\n",
    "\n",
    "\n",
    "# Difference between number of tokens before and after removing stopwords\n",
    "print(\"Difference between number of tokens before and after removing stopwords: \", (unique_words(Copy_tok['content'])-unique_words(Copy_stop['content'])))\n",
    "\n",
    "# number of tokens after tokenizing\n",
    "print(\"Number of words after tokenizing: \", unique_words(Copy_tok['content']))\n",
    "\n",
    "# number of tokens after removing stopwords\n",
    "print(\"Number of words after removing stopwords: \", unique_words(Copy_stop['content']))\n",
    "\n",
    "# reduction rate after removing stopwords\n",
    "print(\"Reduction rate after removing stopwords: \", 100 - unique_words(Copy_stop['content'])/unique_words(Copy_tok['content']) * 100)\n",
    "\n",
    "# number of tokens after stemming\n",
    "print(\"Number of words after stemming: \", unique_words(Copy_stemming['content']))\n",
    "\n",
    "# reduction rate after stemming\n",
    "print(\"Reduction rate after stemming: \", 100 - unique_words(Copy_stemming['content'])/unique_words(Copy_stop['content']) * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The URLs are not correctly formatted: Each URL includes the first word in the content!!!\n",
    "* Not sure whether the counts below are done in the right order\n",
    "* Regarding dates: We need to choose which column to use: (1) scraped_at, (2) inserted_at, (3) updated_at\n",
    "* Insert more \"replace\" statements in CleaningTime (i.e. DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs:  242\n",
      "Number of dates:  27\n",
      "Number of numeric values:  2019\n",
      "Number of punctuations (check):  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # COUNTING\n",
    "# count = 0 \n",
    "# wordlist = ['url', 'number']\n",
    "# for i in Copy_stemming['content']:\n",
    "#     for x in i:\n",
    "#         if x in wordlist:\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             count = count\n",
    "# #count\n",
    "\n",
    "\n",
    "# Count URLs\n",
    "count_url = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'url':\n",
    "            count_url += 1\n",
    "        else:\n",
    "            count_url = count_url\n",
    "print(\"Number of URLs: \", count_url)\n",
    "\n",
    "# Count dates\n",
    "count_date = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'date':\n",
    "            count_date += 1\n",
    "        else:\n",
    "            count_date = count_date\n",
    "print(\"Number of dates: \", count_date)\n",
    "\n",
    "# Count numbers (numeric values)\n",
    "count_number = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'number':\n",
    "            count_number += 1\n",
    "        else:\n",
    "            count_number = count_number\n",
    "print(\"Number of numeric values: \", count_number)\n",
    "\n",
    "# Count punctuation: check if punctuations are removed\n",
    "count_punct = 0\n",
    "for i in Copy_stemming['content']:\n",
    "    for x in i:\n",
    "        if x == 'punct':\n",
    "            count_punct += 1\n",
    "        else:\n",
    "            count_punct = count_punct\n",
    "print(\"Number of punctuations (check): \", count_punct)\n",
    "\n",
    "# # Print the date of the oldest article by using 'inserted_at' column\n",
    "# print(\"The date of the oldest article: \", min(R['inserted_at']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_38944\\1239446524.py:7: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ = pd.read_csv('1mio-raw.csv')\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_38944\\1239446524.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_copy['content'][:100] = df_copy['content'][:100].apply(cleaningTime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   id                domain        type  \\\n",
      "0           0    2         express.co.uk       rumor   \n",
      "1           1    6    barenakedislam.com        hate   \n",
      "2           2    7    barenakedislam.com        hate   \n",
      "3           3    8    barenakedislam.com        hate   \n",
      "4           4    9    barenakedislam.com        hate   \n",
      "..        ...  ...                   ...         ...   \n",
      "95         95  114  bipartisanreport.com   clickbait   \n",
      "96         96  115               awm.com  unreliable   \n",
      "97         97  116               awm.com  unreliable   \n",
      "98         98  117               awm.com  unreliable   \n",
      "99         99  118               awm.com  unreliable   \n",
      "\n",
      "                                                  url  \\\n",
      "0   https://www.express.co.uk/news/science/738402/...   \n",
      "1   http://barenakedislam.com/category/donald-trum...   \n",
      "2   http://barenakedislam.com/category/donald-trum...   \n",
      "3   http://barenakedislam.com/2017/12/24/more-winn...   \n",
      "4   http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
      "..                                                ...   \n",
      "95  http://bipartisanreport.com/2018/01/18/just-in...   \n",
      "96  http://awm.com/driver-films-the-moment-flying-...   \n",
      "97  http://awm.com/model-who-boasts-she-transforme...   \n",
      "98  http://awm.com/after-eating-sushi-man-finds-a-...   \n",
      "99  http://awm.com/youtube-star-claims-because-hot...   \n",
      "\n",
      "                                              content  \\\n",
      "0   life is an illusion at least on a quantum leve...   \n",
      "1   unfortunately he hasnt yet attacked her for is...   \n",
      "2   the los angeles police department has been den...   \n",
      "3   the white house has decided to quietly withdra...   \n",
      "4   the time has come to cut off the tongues of th...   \n",
      "..                                                ...   \n",
      "95  as republicans continue to blast democrats in ...   \n",
      "96  when it snows do you go out early and clear of...   \n",
      "97  martina big a numberyearold white german model...   \n",
      "98  sushi is a trendy dish it comes from japan and...   \n",
      "99  a numberyearold british vlogger has made a nam...   \n",
      "\n",
      "                    scraped_at                 inserted_at  \\\n",
      "0   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "..                         ...                         ...   \n",
      "95  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "96  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "97  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "98  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "99  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                    updated_at  \\\n",
      "0   2018-02-02 01:19:41.756664   \n",
      "1   2018-02-02 01:19:41.756664   \n",
      "2   2018-02-02 01:19:41.756664   \n",
      "3   2018-02-02 01:19:41.756664   \n",
      "4   2018-02-02 01:19:41.756664   \n",
      "..                         ...   \n",
      "95  2018-02-02 01:19:41.756664   \n",
      "96  2018-02-02 01:19:41.756664   \n",
      "97  2018-02-02 01:19:41.756664   \n",
      "98  2018-02-02 01:19:41.756664   \n",
      "99  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                                title  \\\n",
      "0   Is life an ILLUSION? Researchers prove 'realit...   \n",
      "1                                        Donald Trump   \n",
      "2                                        Donald Trump   \n",
      "3   MORE WINNING! Israeli intelligence source, DEB...   \n",
      "4   “Oh, Trump, you coward, you just wait, we will...   \n",
      "..                                                ...   \n",
      "95  JUST IN: Official 2018 Polls Released; America...   \n",
      "96  Driver Films The Moment Flying Object Nearly T...   \n",
      "97  Model Who Boasts She Transformed Into An Afric...   \n",
      "98  After Eating Sushi, Man Finds A Five Foot Crea...   \n",
      "99  Youtube Star Claims Because Hotel Said No To F...   \n",
      "\n",
      "                                              authors  keywords meta_keywords  \\\n",
      "0                                         Sean Martin       NaN          ['']   \n",
      "1   Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN          ['']   \n",
      "2   Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN          ['']   \n",
      "3   Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN          ['']   \n",
      "4   F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN          ['']   \n",
      "..                                                ...       ...           ...   \n",
      "95                                          Holly Lee       NaN          ['']   \n",
      "96                                    Alexander Smith       NaN          ['']   \n",
      "97                                     Wendy Michaels       NaN          ['']   \n",
      "98                                    Alexander Smith       NaN          ['']   \n",
      "99                                    Alexander Smith       NaN          ['']   \n",
      "\n",
      "                                     meta_description tags  summary  source  \n",
      "0   THE UNIVERSE ceases to exist when we are not l...  NaN      NaN     NaN  \n",
      "1                                                 NaN  NaN      NaN     NaN  \n",
      "2                                                 NaN  NaN      NaN     NaN  \n",
      "3                                                 NaN  NaN      NaN     NaN  \n",
      "4                                                 NaN  NaN      NaN     NaN  \n",
      "..                                                ...  ...      ...     ...  \n",
      "95                                                NaN  NaN      NaN     NaN  \n",
      "96                                                NaN  NaN      NaN     NaN  \n",
      "97                                                NaN  NaN      NaN     NaN  \n",
      "98                                                NaN  NaN      NaN     NaN  \n",
      "99                                                NaN  NaN      NaN     NaN  \n",
      "\n",
      "[100 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "#csv = open('news_cleaned_2018_02_13.csv')\n",
    "#csv_ = open('1mio-raw.csv')\n",
    "#df = pd.read_csv('news_cleaned_2018_02_13.csv')\n",
    "#df = pd.read_csv('news_cleaned_2018_02_13.csv',sep=',', header = None, skiprows=1000, chunksize=10)\n",
    "df_ = pd.read_csv('1mio-raw.csv')\n",
    "\n",
    "# apply cleaningtime function to the content column on a copy of the dataframe\n",
    "# make deep copy of the dataframe\n",
    "df_copy = df_.copy(deep=True)\n",
    "# apply cleaningtime function to the content column. Handle attrribute error float has no attribute replace\n",
    "for x in df_copy['content'][:100]:\n",
    "    try:\n",
    "        df_copy['content'][:100] = df_copy['content'][:100].apply(cleaningTime)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(df_copy.head(100)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      life is an illusion at least on a quantum leve...\n",
      "1      unfortunately he hasnt yet attacked her for is...\n",
      "2      the los angeles police department has been den...\n",
      "3      the white house has decided to quietly withdra...\n",
      "4      the time has come to cut off the tongues of th...\n",
      "                             ...                        \n",
      "97     martina big a numberyearold white german model...\n",
      "98     sushi is a trendy dish it comes from japan and...\n",
      "99     a numberyearold british vlogger has made a nam...\n",
      "100    (Facebook/Support Kountze Kids Faith)\\n\\nA Tex...\n",
      "101    Concentration Camps for African Migrants Block...\n",
      "Name: content, Length: 102, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_copy.head(102)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/news/science/738402/...</td>\n",
       "      <td>Life is an illusion, at least on a quantum lev...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>The Los Angeles Police Department has been den...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n",
       "      <td>The White House has decided to quietly withdra...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n",
       "      <td>“The time has come to cut off the tongues of t...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n",
       "      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>9929</td>\n",
       "      <td>1170116</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976IBA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976IBADAN00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>9930</td>\n",
       "      <td>1170117</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976BOM...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976BOMBAY00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>9931</td>\n",
       "      <td>1170118</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>9932</td>\n",
       "      <td>1170119</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974BAN...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1974BANGKO15999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>9933</td>\n",
       "      <td>1170120</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1974STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       id              domain        type  \\\n",
       "0               0        2       express.co.uk       rumor   \n",
       "1               1        6  barenakedislam.com        hate   \n",
       "2               2        7  barenakedislam.com        hate   \n",
       "3               3        8  barenakedislam.com        hate   \n",
       "4               4        9  barenakedislam.com        hate   \n",
       "...           ...      ...                 ...         ...   \n",
       "999995       9929  1170116       wikileaks.org  unreliable   \n",
       "999996       9930  1170117       wikileaks.org  unreliable   \n",
       "999997       9931  1170118       wikileaks.org  unreliable   \n",
       "999998       9932  1170119       wikileaks.org  unreliable   \n",
       "999999       9933  1170120       wikileaks.org  unreliable   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://www.express.co.uk/news/science/738402/...   \n",
       "1       http://barenakedislam.com/category/donald-trum...   \n",
       "2       http://barenakedislam.com/category/donald-trum...   \n",
       "3       http://barenakedislam.com/2017/12/24/more-winn...   \n",
       "4       http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
       "...                                                   ...   \n",
       "999995  https://www.wikileaks.org/plusd/cables/1976IBA...   \n",
       "999996  https://www.wikileaks.org/plusd/cables/1976BOM...   \n",
       "999997  https://www.wikileaks.org/plusd/cables/1976STA...   \n",
       "999998  https://www.wikileaks.org/plusd/cables/1974BAN...   \n",
       "999999  https://www.wikileaks.org/plusd/cables/1974STA...   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Life is an illusion, at least on a quantum lev...   \n",
       "1       Unfortunately, he hasn’t yet attacked her for ...   \n",
       "2       The Los Angeles Police Department has been den...   \n",
       "3       The White House has decided to quietly withdra...   \n",
       "4       “The time has come to cut off the tongues of t...   \n",
       "...                                                   ...   \n",
       "999995  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999996  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999997  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999998  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999999  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "\n",
       "                        scraped_at                 inserted_at  \\\n",
       "0       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "...                            ...                         ...   \n",
       "999995  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999996  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999997  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999998  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999999  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                        updated_at  \\\n",
       "0       2018-02-02 01:19:41.756664   \n",
       "1       2018-02-02 01:19:41.756664   \n",
       "2       2018-02-02 01:19:41.756664   \n",
       "3       2018-02-02 01:19:41.756664   \n",
       "4       2018-02-02 01:19:41.756664   \n",
       "...                            ...   \n",
       "999995  2018-02-02 01:19:41.756664   \n",
       "999996  2018-02-02 01:19:41.756664   \n",
       "999997  2018-02-02 01:19:41.756664   \n",
       "999998  2018-02-02 01:19:41.756664   \n",
       "999999  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                            Donald Trump   \n",
       "2                                            Donald Trump   \n",
       "3       MORE WINNING! Israeli intelligence source, DEB...   \n",
       "4       “Oh, Trump, you coward, you just wait, we will...   \n",
       "...                                                   ...   \n",
       "999995                             Cable: 1976IBADAN00030   \n",
       "999996                             Cable: 1976BOMBAY00030   \n",
       "999997                             Cable: 1976STATE087319   \n",
       "999998                             Cable: 1974BANGKO15999   \n",
       "999999                             Cable: 1974STATE087319   \n",
       "\n",
       "                                                  authors  keywords  \\\n",
       "0                                             Sean Martin       NaN   \n",
       "1       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "2       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "3       Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN   \n",
       "4       F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN   \n",
       "...                                                   ...       ...   \n",
       "999995                                                NaN       NaN   \n",
       "999996                                                NaN       NaN   \n",
       "999997                                                NaN       NaN   \n",
       "999998                                                NaN       NaN   \n",
       "999999                                                NaN       NaN   \n",
       "\n",
       "       meta_keywords                                   meta_description  \\\n",
       "0               ['']  THE UNIVERSE ceases to exist when we are not l...   \n",
       "1               ['']                                                NaN   \n",
       "2               ['']                                                NaN   \n",
       "3               ['']                                                NaN   \n",
       "4               ['']                                                NaN   \n",
       "...              ...                                                ...   \n",
       "999995          ['']                                                NaN   \n",
       "999996          ['']                                                NaN   \n",
       "999997          ['']                                                NaN   \n",
       "999998          ['']                                                NaN   \n",
       "999999          ['']                                                NaN   \n",
       "\n",
       "             tags  summary  source  \n",
       "0             NaN      NaN     NaN  \n",
       "1             NaN      NaN     NaN  \n",
       "2             NaN      NaN     NaN  \n",
       "3             NaN      NaN     NaN  \n",
       "4             NaN      NaN     NaN  \n",
       "...           ...      ...     ...  \n",
       "999995  View Tags      NaN     NaN  \n",
       "999996  View Tags      NaN     NaN  \n",
       "999997  View Tags      NaN     NaN  \n",
       "999998  View Tags      NaN     NaN  \n",
       "999999  View Tags      NaN     NaN  \n",
       "\n",
       "[1000000 rows x 17 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_.drop('id', inplace=True, axis=1)\n",
    "#df_.drop('inserted_at', inplace=True, axis=1)\n",
    "#df_.drop('scraped_at', inplace=True, axis=1)\n",
    "#df_.drop('updated_at', inplace=True, axis=1)\n",
    "#df_.drop('summary', inplace=True, axis=1)\n",
    "#df_.drop('tags', inplace=True, axis=1)\n",
    "#df_.drop('source', inplace=True, axis=1)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/news/science/738402/...</td>\n",
       "      <td>Life is an illusion, at least on a quantum lev...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>The Los Angeles Police Department has been den...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n",
       "      <td>The White House has decided to quietly withdra...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n",
       "      <td>“The time has come to cut off the tongues of t...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n",
       "      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>9929</td>\n",
       "      <td>1170116</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976IBA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976IBADAN00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>9930</td>\n",
       "      <td>1170117</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976BOM...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976BOMBAY00030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>9931</td>\n",
       "      <td>1170118</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1976STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1976STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>9932</td>\n",
       "      <td>1170119</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974BAN...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1974BANGKO15999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>9933</td>\n",
       "      <td>1170120</td>\n",
       "      <td>wikileaks.org</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>https://www.wikileaks.org/plusd/cables/1974STA...</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable: 1974STATE087319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>View Tags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       id              domain        type  \\\n",
       "0               0        2       express.co.uk       rumor   \n",
       "1               1        6  barenakedislam.com        hate   \n",
       "2               2        7  barenakedislam.com        hate   \n",
       "3               3        8  barenakedislam.com        hate   \n",
       "4               4        9  barenakedislam.com        hate   \n",
       "...           ...      ...                 ...         ...   \n",
       "999995       9929  1170116       wikileaks.org  unreliable   \n",
       "999996       9930  1170117       wikileaks.org  unreliable   \n",
       "999997       9931  1170118       wikileaks.org  unreliable   \n",
       "999998       9932  1170119       wikileaks.org  unreliable   \n",
       "999999       9933  1170120       wikileaks.org  unreliable   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://www.express.co.uk/news/science/738402/...   \n",
       "1       http://barenakedislam.com/category/donald-trum...   \n",
       "2       http://barenakedislam.com/category/donald-trum...   \n",
       "3       http://barenakedislam.com/2017/12/24/more-winn...   \n",
       "4       http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
       "...                                                   ...   \n",
       "999995  https://www.wikileaks.org/plusd/cables/1976IBA...   \n",
       "999996  https://www.wikileaks.org/plusd/cables/1976BOM...   \n",
       "999997  https://www.wikileaks.org/plusd/cables/1976STA...   \n",
       "999998  https://www.wikileaks.org/plusd/cables/1974BAN...   \n",
       "999999  https://www.wikileaks.org/plusd/cables/1974STA...   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Life is an illusion, at least on a quantum lev...   \n",
       "1       Unfortunately, he hasn’t yet attacked her for ...   \n",
       "2       The Los Angeles Police Department has been den...   \n",
       "3       The White House has decided to quietly withdra...   \n",
       "4       “The time has come to cut off the tongues of t...   \n",
       "...                                                   ...   \n",
       "999995  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999996  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999997  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999998  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "999999  Tor\\n\\nTor is an encrypted anonymising network...   \n",
       "\n",
       "                        scraped_at                 inserted_at  \\\n",
       "0       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "...                            ...                         ...   \n",
       "999995  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999996  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999997  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999998  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "999999  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                        updated_at  \\\n",
       "0       2018-02-02 01:19:41.756664   \n",
       "1       2018-02-02 01:19:41.756664   \n",
       "2       2018-02-02 01:19:41.756664   \n",
       "3       2018-02-02 01:19:41.756664   \n",
       "4       2018-02-02 01:19:41.756664   \n",
       "...                            ...   \n",
       "999995  2018-02-02 01:19:41.756664   \n",
       "999996  2018-02-02 01:19:41.756664   \n",
       "999997  2018-02-02 01:19:41.756664   \n",
       "999998  2018-02-02 01:19:41.756664   \n",
       "999999  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                            Donald Trump   \n",
       "2                                            Donald Trump   \n",
       "3       MORE WINNING! Israeli intelligence source, DEB...   \n",
       "4       “Oh, Trump, you coward, you just wait, we will...   \n",
       "...                                                   ...   \n",
       "999995                             Cable: 1976IBADAN00030   \n",
       "999996                             Cable: 1976BOMBAY00030   \n",
       "999997                             Cable: 1976STATE087319   \n",
       "999998                             Cable: 1974BANGKO15999   \n",
       "999999                             Cable: 1974STATE087319   \n",
       "\n",
       "                                                  authors  keywords  \\\n",
       "0                                             Sean Martin       NaN   \n",
       "1       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "2       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n",
       "3       Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN   \n",
       "4       F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN   \n",
       "...                                                   ...       ...   \n",
       "999995                                                NaN       NaN   \n",
       "999996                                                NaN       NaN   \n",
       "999997                                                NaN       NaN   \n",
       "999998                                                NaN       NaN   \n",
       "999999                                                NaN       NaN   \n",
       "\n",
       "       meta_keywords                                   meta_description  \\\n",
       "0               ['']  THE UNIVERSE ceases to exist when we are not l...   \n",
       "1               ['']                                                NaN   \n",
       "2               ['']                                                NaN   \n",
       "3               ['']                                                NaN   \n",
       "4               ['']                                                NaN   \n",
       "...              ...                                                ...   \n",
       "999995          ['']                                                NaN   \n",
       "999996          ['']                                                NaN   \n",
       "999997          ['']                                                NaN   \n",
       "999998          ['']                                                NaN   \n",
       "999999          ['']                                                NaN   \n",
       "\n",
       "             tags  summary  source  label  \n",
       "0             NaN      NaN     NaN      1  \n",
       "1             NaN      NaN     NaN      1  \n",
       "2             NaN      NaN     NaN      1  \n",
       "3             NaN      NaN     NaN      1  \n",
       "4             NaN      NaN     NaN      1  \n",
       "...           ...      ...     ...    ...  \n",
       "999995  View Tags      NaN     NaN      1  \n",
       "999996  View Tags      NaN     NaN      1  \n",
       "999997  View Tags      NaN     NaN      1  \n",
       "999998  View Tags      NaN     NaN      1  \n",
       "999999  View Tags      NaN     NaN      1  \n",
       "\n",
       "[1000000 rows x 18 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new column 'label' and assign 1 to all rows with 'type' = 'fake' or 'conspiracy' or 'bias' or 'junksci' or 'hate' or 'unreliable' or 'rumor' and 0 to all other rows\n",
    "df_['label'] = df_['type'].apply(lambda x: 1 if x == 'fake' or x == 'conspiracy' or x == 'bias' or x == 'junksci' or x == 'hate' or x == 'unreliable' or x == 'rumor' else 0)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579268"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.tail(20)\n",
    "count = 0\n",
    "for i in df_['label']:\n",
    "    if i == 1:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcleantext\u001b[39;00m \u001b[39mimport\u001b[39;00m clean\n\u001b[0;32m      2\u001b[0m \u001b[39m# cleaning the train set\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(cleaningTime)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[79], line 23\u001b[0m, in \u001b[0;36mcleaningTime\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcleaningTime\u001b[39m(x):\n\u001b[1;32m---> 23\u001b[0m     clean_text \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     clean_text \u001b[39m=\u001b[39m clean_text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     regex \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[xX]+\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "# cleaning the train set\n",
    "df_['content'] = df_['content'].apply(cleaningTime)\n",
    "\n",
    "# cleaning the train set with error handling (if is in instance of string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 18)\n",
      "(100000, 18)\n",
      "(100000, 18)\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset into train, validation and test set\n",
    "train_set = df_.sample(frac=0.8, random_state=0)\n",
    "new_df = df_.drop(train_set.index)\n",
    "validation_set = new_df.sample(frac=0.5, random_state=0)\n",
    "test_set = new_df.drop(validation_set.index)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(validation_set.shape)\n",
    "print(test_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# cleaning the train set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_set[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_set[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(cleaningTime)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[79], line 23\u001b[0m, in \u001b[0;36mcleaningTime\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcleaningTime\u001b[39m(x):\n\u001b[1;32m---> 23\u001b[0m     clean_text \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     clean_text \u001b[39m=\u001b[39m clean_text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     regex \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[xX]+\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m      9\u001b[0m \u001b[39m# tokenize and build vocab\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m vectorizer\u001b[39m.\u001b[39;49mfit(train_set[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     11\u001b[0m \u001b[39m# summarize\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, raw_documents, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[39m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1275\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:239\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# baseline model using count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(train_set['content'])\n",
    "# summarize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "900b9bb925fcdb02ec6c2453d36a87d904f6c7d996c9903f56d484ca6ecf1666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
