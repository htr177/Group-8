{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Scraping One Page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 & 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://www.bbc.com/news')\n",
    "contents = response.text\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "# print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing the whole HTML file, I searched for the title \"bloodshed\" for verification and was successfully located."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 & 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukraine orders evacuation of city it recaptured\n",
      "Ukraine orders evacuation of city it recaptured\n",
      "US lawyer Murdaugh guilty of killing wife and son\n",
      "How Alex Murdaugh hid his dark side\n",
      "Cambodian opposition leader sentenced for treason\n",
      "Putin accuses Ukraine of border 'terrorist act'\n",
      "Tennessee bans drag shows for children\n",
      "Ros Atkins on... The creeping TikTok bans\n",
      "Half of world could be overweight by 2035\n",
      "At least 57 confirmed dead in Greece train crash\n",
      "Scotland first to ban anaesthetic over environment\n",
      "Text leak puts spotlight on police and quarantine in UK \n",
      "Scotland first to ban anaesthetic over environment\n",
      "Text leak puts spotlight on police and quarantine in UK \n",
      "Jazz saxophone legend Wayne Shorter dies at 89\n",
      "Teenager bitten by crocodile in Australian floods\n",
      "India actor banned from stock market over malpractice\n",
      "Hong Kong skyscraper fire seen on city's skyline\n",
      "BBC World News TV\n",
      "BBC World Service Radio\n",
      "Man survives 31 days in jungle by eating worms\n",
      "Egypt pyramid hidden corridor seen for first time\n",
      "Weekly quiz: Which house are Harry and Meghan vacating?\n",
      "Should animals be kept in zoos?\n",
      "Pet crocs and red carpets: Africa's top shots\n",
      "What we've learned from Alex Murdaugh murder trial\n",
      "Robotaxi tech improves but can they make money?\n",
      "Why half of India's urban women stay at home\n",
      "Lab leak divisions toxify Covid origins search\n",
      "Listen: How long will US support for Kyiv last?\n",
      "Mental health gyms: A place to train your brain\n",
      "Where do Harry and Meghan get their money?\n",
      "The largest objects in the Universe\n",
      "The rise of the Gen Z side hustle\n",
      "A blouse that unites Southeast Asia\n",
      "The most shocking Oscars moment ever\n",
      "Why trees are being stolen for firewood\n",
      "The problem confronting women of colour\n",
      "How three widows came to rule Champagne\n",
      "Roy hits ton as brilliant England post 326-7 against Bangladesh\n",
      "From peacetime team-mates to wartime enemies\n",
      "Yorkshire racism hearing - follow day three live\n",
      "Premier League news conferences: 'Man Utd are results machine' - Klopp\n",
      "Australia beat India to seal Test Championship spot\n",
      "Barca beat Madrid in Copa del Rey semi-final first leg\n",
      "Canada women agree interim deal in equal pay fight\n"
     ]
    }
   ],
   "source": [
    "# Extract all headers from the page with a regular expression\n",
    "\n",
    "import re\n",
    "\n",
    "headers = re.compile(\"(<h3.+promo.+>)+(.+)(</h3>)\")\n",
    "headers1 = soup.findAll(\"h3\")\n",
    "\n",
    "for item in headers1:\n",
    "    try:\n",
    "        print(headers.match(f\"{item}\").groups()[1])\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I was unable to retrieve relevant header tags. For example, \"BBC World News TV\" is not a header we are interested in. The RE I used also captures media links (videos). In general, using regular expressions should be avoided since HTML language is unstable/varying (i.e. internet sites are regularly updated), and would therefore require frequent maintenance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5 & 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victory is inevitable if allies keep promises - Zelensky\n",
      "WATCH: One year of war in Ukraine in 87 seconds\n",
      "Fighting to stay Ukrainian in a frontline mining town\n",
      "BBC correspondents reflect on a year of warzone reporting\n",
      "Has Putin's war failed?\n",
      "Two friends changed by a year of war\n",
      "How Putin's fate is tied to his war in Ukraine\n",
      "Why China launched a charm offensive over Ukraine\n",
      "US marks war anniversary with new Russia sanctions\n",
      "Brothers leave Guantanamo Bay after almost 20 years\n",
      "US ex-lawyer Alex Murdaugh admits he stole millions for drugs\n",
      "Swimmers 'ruined' by fat-shaming and bullying\n",
      "Moldova warns of Russian 'psy-ops' as tensions rise\n",
      "Rebellious Andean bear sneaks out of US zoo - twice\n",
      "Kenyan man freed over Britons' murder-kidnap\n",
      "Nigerian politician arrested with $500,000 in cash\n",
      "Netflix cuts prices in more than 30 countries\n",
      "Nigerian politician arrested with $500,000 in cash\n",
      "Netflix cuts prices in more than 30 countries\n",
      "US billionaire financier Thomas Lee found dead at 78\n",
      "Seoul offers radiation tests to N Korea defectors\n",
      "Cambodian girl dies in rare bird flu case\n"
     ]
    }
   ],
   "source": [
    "# Find top stories with soup.find\n",
    "\n",
    "top = soup.find(id=\"news-top-stories-container\")\n",
    "\n",
    "h3 = top.find_all(\"h3\")\n",
    "\n",
    "for item in h3:\n",
    "    try:\n",
    "        print(headers.match(f\"{item}\").groups()[1])\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the first German-made tanks arrive in Ukraine, President Zelensky urges allies to stick to their promises and deadlines.\n",
      "Steve Rosenberg looks at why Vladimir Putin set sail in a storm of his own making a year ago.\n",
      "The West may come away unimpressed - but convincing them was never likely the main goal for Beijing.\n",
      "President Biden also announced over $2bn in military aid to both Ukraine and neighbouring Moldova.\n",
      "Abdul and Mohammed Ahmed Rabbani were arrested in Pakistan in 2002. They were never charged by the US.\n",
      "The former lawyer accused of murdering his wife and son had a powerful 60-pill-a-day opiate habit.\n",
      "Former athletes tell of mistreatment at clubs across England, with allegations stretching back more than a decade.\n",
      "Moldova's pro-EU leaders reject Russian claims that Ukraine plans to attack its breakaway territory.\n",
      "The South American species escaped his habitat at the St Louis Zoo for the second time this month.\n",
      "The BBC revealed last year that a senior Met officer who assisted the Kenyan investigation \"omitted evidence\".\n",
      "The money was found in Chinyere Igwe's car on the eve of Nigeria's hotly contested elections.\n",
      "The streaming giant has faced increasing competition from rivals including Amazon, HBO and Disney.\n",
      "The money was found in Chinyere Igwe's car on the eve of Nigeria's hotly contested elections.\n",
      "The streaming giant has faced increasing competition from rivals including Amazon, HBO and Disney.\n",
      "The private equity pioneer died from a self-inflicted gunshot wound at his Manhattan office, local media report.\n",
      "A Seoul-based rights group said leakage of radioactive materials into groundwater is a major concern.\n",
      "It is the first known human transmission of bird flu in the South East Asian nation since 2014.\n",
      "The latest global news, sport, weather and documentaries\n",
      "Stories from around the world\n"
     ]
    }
   ],
   "source": [
    "# Finding summaries\n",
    "\n",
    "summaries = re.compile(\"(<p.+promo.+>)+(.+)(</p>)\")\n",
    "summaries1 = soup.findAll(\"p\")\n",
    "\n",
    "for item in summaries1:\n",
    "    try:\n",
    "        print(summaries.match(f\"{item}\").groups()[1])\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7 & 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Ukraine orders evacuation of city it recaptured\n",
      "Summary: Families are told to leave Kupiansk, which Ukraine re-captured from Russia in September.\n",
      "Section: Europe\n",
      "------------------\n",
      "Headline: US lawyer Murdaugh guilty of killing wife and son\n",
      "Summary: \"The evidence of guilt is overwhelming,\" the judge says after the jury's verdict is read in court.\n",
      "Section: US & Canada\n",
      "------------------\n",
      "Headline: How Alex Murdaugh hid his dark side\n",
      "Summary: Behind the courtly air of a country lawyer born to power and privilege, lay a cold-blooded killer.\n",
      "Section: US & Canada\n",
      "------------------\n",
      "Headline: Cambodian opposition leader sentenced for treason\n",
      "Summary: Kem Sokha is sentenced to 27 years house arrest which prevents him from contesting July's election.\n",
      "Section: Asia\n",
      "------------------\n",
      "Headline: Putin accuses Ukraine of border 'terrorist act'\n",
      "Summary: Kyiv denies Moscow's claim that Ukrainian saboteurs fired at civilians in a Russian village.\n",
      "Section: Europe\n",
      "------------------\n",
      "Headline: Tennessee bans drag shows for children\n",
      "Summary: Violators of the new drag law face nearly a year in jail and a fine of up to $2,500.\n",
      "Section: US & Canada\n",
      "------------------\n",
      "Headline: Ros Atkins on... The creeping TikTok bans\n",
      "Summary: The app has been banned from employee phones by various governments amid security concerns.\n",
      "Section: Technology\n",
      "------------------\n",
      "Headline: Half of world could be overweight by 2035\n",
      "Summary: Africa and Asia are expected to see the biggest rises in obesity, the World Obesity Federation says.\n",
      "Section: World\n",
      "------------------\n",
      "Headline: At least 57 confirmed dead in Greece train crash\n",
      "Summary: A coroner tells the BBC that DNA has been collected from 57 bodies, as rail workers strike over the disaster.\n",
      "Section: Europe\n",
      "------------------\n",
      "Headline: Scotland first to ban anaesthetic over environment\n",
      "Summary: Desflurane gas has a global warming potential 2,500 times greater than carbon dioxide, NHS data suggests.\n",
      "Section: Health\n",
      "------------------\n",
      "Headline: Text leak puts spotlight on police and quarantine in UK \n",
      "Summary: The latest messages show Matt Hancock and his aide discussing people \"locked up\" in hotel \"box rooms\".\n",
      "Section: UK Politics\n",
      "------------------\n",
      "Headline: Jazz saxophone legend Wayne Shorter dies at 89\n",
      "Summary: The 12-time Grammy award winner is credited with shaping much of 20th Century jazz music.\n",
      "Section: US & Canada\n",
      "------------------\n",
      "Headline: Teenager bitten by crocodile in Australian floods\n",
      "Summary: The 17-year-old boy was attacked while being evacuated from a remote community following heavy rain.\n",
      "Section: Australia\n",
      "------------------\n",
      "Headline: India actor banned from stock market over malpractice\n",
      "Summary: India's market regulator says they were involved in share price manipulation.\n",
      "Section: India\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# Combine headers, summaries and sections into a list of dictionaries\n",
    "\n",
    "combined = re.compile(\"(<div.+body.+>)(.+)(</h3></a>.+)(promo.summary.>)+(.+)(</p>.+)(<span.+true.>)([^<]+)(</span>.+)\")\n",
    "\n",
    "stories = []\n",
    "\n",
    "for item in soup.find_all(\"div\", {\"class\": \"gs-c-promo\"}):\n",
    "    try:\n",
    "        headline = combined.match(str(item)).groups()[1]\n",
    "        summary = combined.match(str(item)).groups()[4]\n",
    "        section = combined.match(str(item)).groups()[7].replace(\"&amp;\", \"&\")\n",
    "        story = {'headline': headline, 'summary': summary, 'section': section}\n",
    "        if not any(s['headline'] == headline for s in stories):\n",
    "            stories.append(story)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "for story in stories:\n",
    "    print('Headline:', story['headline'])\n",
    "    print('Summary:', story['summary'])\n",
    "    print('Section:', story['section'])\n",
    "    print('------------------')\n",
    "\n",
    "# create a json file for storing the dictionaries\n",
    "\n",
    "import json\n",
    "\n",
    "with open('bbc.json', 'w') as f:\n",
    "    json.dump(stories, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Scraping a Reliable News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "group_nr = 8\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "with open('article_titles_links.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    output_strings = []\n",
    "    printed_categories = set()\n",
    "    for letter in letters:\n",
    "        url = f\"https://en.wikinews.org/wiki/Category:Politics_and_conflicts?from={letter}&to={letters[-1]}\"\n",
    "        while url:\n",
    "            category_request = requests.get(url)\n",
    "            contents = category_request.text\n",
    "            category_soup = BeautifulSoup(contents, 'html.parser')\n",
    "            category_section = category_soup.find(id=\"mw-pages\")\n",
    "            subcategories = category_section.find_all('a', title=True)\n",
    "            for subcategory in subcategories:\n",
    "                subcategory_name = subcategory['title']\n",
    "                subcategory_name = subcategory_name.replace('\"', '')\n",
    "                if subcategory_name in printed_categories or subcategory_name.startswith('Q') or subcategory_name.startswith(\"'\"):\n",
    "                    continue\n",
    "    \n",
    "                printed_categories.add(subcategory_name)\n",
    "                if subcategory_name[0] > letters[-1]:\n",
    "                    \n",
    "                    break\n",
    "                subcategory_url = f\"https://en.wikinews.org{subcategory['href']}\"\n",
    "\n",
    "                if 'pageto' not in subcategory_url and 'pagefrom' not in subcategory_url:\n",
    "                    output_str = f\"{subcategory_name},{subcategory_url}\\n\"\n",
    "                    output_strings.append(output_str)         \n",
    "\n",
    "            next_link = category_soup.find(\"a\", string=\"next page\")\n",
    "            if next_link:\n",
    "                url = f\"https://en.wikinews.org{next_link['href']}\"\n",
    "            else:\n",
    "                url = None\n",
    "\n",
    "    output_strings.sort()\n",
    "    for output_str in output_strings:\n",
    "        f.write(output_str)\n",
    "\n",
    "# Number of articles\n",
    "print(len(output_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_strings = [output_str.strip() for output_str in output_strings]\n",
    "\n",
    "output_lists = [output_str.split(',') for output_str in output_strings]\n",
    "\n",
    "output_tuples = [tuple(output_str.split(',')) for output_str in output_strings]\n",
    "\n",
    "def res(): \n",
    "    lst = []\n",
    "    for ele in output_strings:\n",
    "        peter = re.search(r'https:.+', ele)\n",
    "        peter = peter.group(0)\n",
    "        lst.append(peter)\n",
    "    return lst\n",
    "\n",
    "urls = [output_str.split(',') for output_str in output_strings]\n",
    "urls = res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(inp):\n",
    "    lst = []\n",
    "    for elm in inp:\n",
    "        i = inp.index(elm)\n",
    "        response = requests.get(elm)\n",
    "        contents = response.text\n",
    "        soup = BeautifulSoup(contents, 'html.parser')\n",
    "        lst.append([get_article_title(soup), urls[i], get_article_date(soup) , get_article_info(soup)])\n",
    "        i =+ 1\n",
    "    return lst    \n",
    "\n",
    "def get_article_title(inp):\n",
    "    n = inp.find('span', class_='mw-page-title-main')\n",
    "    n = str(n)\n",
    "    n = re.sub(r'<span(?:.*?)>', '', n)\n",
    "    n = re.sub(r'</span>', '', n)\n",
    "    return n\n",
    "\n",
    "def get_article_date(inp):\n",
    "    n = inp.find('strong', class_='published')\n",
    "    n = str(n)\n",
    "    n = re.sub(r'<strong(?:.*?)</span>', '', n)\n",
    "    n = re.sub(r'</strong>', '', n)\n",
    "    return n\n",
    "\n",
    "def get_article_info(inp):\n",
    "    n = inp.find('div', class_='mw-parser-output')\n",
    "    n = inp.find_all('p')\n",
    "    ntext = ''\n",
    "    for elm in n:\n",
    "        ntext += elm.text\n",
    "    n = ntext\n",
    "    n = re.sub(r'\\n', '', n)\n",
    "    n = re.sub(r'\\\"', '', n)\n",
    "    n = re.sub(r'\\[', '', n)\n",
    "    n = re.sub(r'\\]', '', n)\n",
    "    n = re.sub(r'Share.+', '', n)\n",
    "    n = re.sub(r'\\xa0', '', n)\n",
    "    n = re.sub(r'\\w+, \\w+ \\d+, \\d+', '', n) \n",
    "    return n\n",
    "\n",
    "with open('articles.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Content'])\n",
    "    writer.writerows(get_article_content(res()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "One of the challenges with this assignment came up when I tried to retrieve the article links in a correct form for later use. Whenever there was a \",_\" in an article URL, the hyperlink would be inactive and could therefore not be used for scraping. I was then able to get around this problem and created a csv file (article_titles_links.csv) with the article titles and their respective URLs. There were also other minor problems along the way that required hard-coding.\n",
    "\n",
    "The number of articles is calculated to be 2869. In order to get a desired output, I relied on csv file creations and notably a functionality to deal with retrieving links on subsequent pages (next_link). The first part is maybe too extensive considering the output, while the last part of the code creates separate functions for articale title, date, content, and a mother function that incorporates these functions and appends to a list to give the final output. For this part, I was heavily dependent on regular expressions.\n",
    "\n",
    "Scraping from Wikinews does not give me any decisive indication on whether we are dealing with trusted news articles or not. The fact that wikinews can be edited by anyone can be deemed 'sketchy', however the opposite can also be argued since the whole public potentially can edit this content, and not just some specific entity with vested interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b520166f51b3739a5356acde9088e5ad78b24451cde60c1e0e825427624b88d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
